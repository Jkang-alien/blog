---
title: Information
author: JK
date: '2020-05-31'
slug: information
categories:
  - Math
  - Information Theory
tags:
  - Information theory
  - Math
  - Machine learning
subtitle: ''
summary: ''
authors: []
lastmod: '2020-05-31T07:44:56+09:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<p>Information relates to uncertainty. The Shannon information content of an outcome <span class="math inline">\(x\)</span> is <span class="math inline">\(h(x)=-log_{2}P(x)\)</span>. The rare event has larger information than a common event. The unit of information is a bit (binary digit). Coding is a mapping from an outcome of an ensemble to binary digits <span class="math inline">\(\{0,1\}^+\)</span>. A symbol code is a code for a <strong>single</strong> ensemble. A block code is a code for a <strong>sequence</strong> ensemble. A set of sequences of the ensemble has a typical subset. The cardinality of a typical set is <span class="math inline">\(2^{H_{2}X}\)</span>. We can reduce a code length by mapping codes to only a typical set (the source coding theorem). The prefix code is an optimal symbol code. The Kraft inequality is the condition of prefix code <span class="math inline">\(\Sigma_{i}2^{-l_{i}} \le 1\)</span>.</p>
<p>The noisy-channel coding theorem describes the possible rate and block code length <span class="math inline">\(N\)</span>. If the block code length <span class="math inline">\(N\)</span> is long enough, the channel looks like the noisy typewriter and arbitrary block error rate can be achieved with rate. The maximum rate is the capacity <span class="math inline">\(C\)</span> of the channel. If the rate is small enough, the typical set of the output of the channel can be mapped for the typical set of input without overlap.</p>

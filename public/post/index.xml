<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Jun&#39;s Blog</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 22 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Rstudio Conference Global 2021</title>
      <link>/post/2021-01-22-rstudio-conference-global-2021/</link>
      <pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/post/2021-01-22-rstudio-conference-global-2021/</guid>
      <description>


&lt;div id=&#34;max-kuhn&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Max Kuhn&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;New tuning methods
&lt;ul&gt;
&lt;li&gt;Racing method&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;section&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;&lt;/h4&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Convergence</title>
      <link>/post/2020-12-29-extended-non-negative-real-line-and-limit/</link>
      <pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-12-29-extended-non-negative-real-line-and-limit/</guid>
      <description>


&lt;p&gt;It is the main subject of analysis that finding conditions making sequential mathematical objects like a set, sequence, series to be convergent. Induction changes &lt;span class=&#34;math inline&#34;&gt;\(S = \mathbb{N}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(s_{1} \in S\)&lt;/span&gt; and
if $ s_{n} S$ then &lt;span class=&#34;math inline&#34;&gt;\(s_{n+1} \in S\)&lt;/span&gt;. The natural number has a property of endless addable with one. But, induction can prove only natural number &lt;span class=&#34;math inline&#34;&gt;\(\mathbb {N}\)&lt;/span&gt; not infinity &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
Induction \\
s_{1} \in S \\
if\ s_{n} \in S \ then \ s_{n+1} \in S \\
Then\ S = \mathbb{N} \\
\]&lt;/span&gt;
The limit is the way &lt;span class=&#34;math inline&#34;&gt;\(\mathbb {N}\)&lt;/span&gt; goes to &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt;. But the limit operation should be justified by an axiom or a proof. &lt;span class=&#34;math inline&#34;&gt;\(\bigcup^{\infty}_{n=1} U\)&lt;/span&gt; is open, where &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; is open in topology. In the extended non-negative real line, an infinite series &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{n=1}^{\infty} x_{n} \in [0, \infty]\)&lt;/span&gt; is always convergent as a limit of the partial sum &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{n=1}^{N} x_{n}\)&lt;/span&gt;. In the sequence version, a sequence (&lt;span class=&#34;math inline&#34;&gt;\(a_{n}\)&lt;/span&gt;) is converges to real number if there exist &lt;span class=&#34;math inline&#34;&gt;\(N \in \mathbb {N}\)&lt;/span&gt; such that for every &lt;span class=&#34;math inline&#34;&gt;\(n \ge N\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lvert a_{n} - a \rvert &amp;lt; \epsilon\)&lt;/span&gt; for every &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt;. This is the point where natural number &lt;span class=&#34;math inline&#34;&gt;\(\mathbb N\)&lt;/span&gt; applies it’s property of endless addable with one. The &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; can be replaced by neiborhood in topological space.&lt;/p&gt;
&lt;p&gt;The series of &lt;span class=&#34;math inline&#34;&gt;\(a_{n}\)&lt;/span&gt; can be a series or just a set of a complex number &lt;span class=&#34;math inline&#34;&gt;\(c \in \mathbb {C}\)&lt;/span&gt;, function, set, integration, differentiation, or other mathematical objects. But computing the limit is different by how the sequence or the set is processed. If the sequence is processed by union of sets, the limit is defined by computing element-wise limit &lt;span class=&#34;math inline&#34;&gt;\(\bigcap^\infty_{n=1} A_n = \{x|x \in A_n \ for \ all \ n \in \mathbb{N} \}\)&lt;/span&gt;. Induction can not apply to the limit &lt;span class=&#34;math inline&#34;&gt;\(\bigcap_{n=1}^N A_n = \{x|x \in A_n \ for \ all \ n \in N\}\)&lt;/span&gt;. The integration is defined by supremum of a set of simple function integral &lt;span class=&#34;math inline&#34;&gt;\(Simp\ \int_{R^d} f(x) dx := c_1 m(E_1) + ... + c_k (E_k)\)&lt;/span&gt;. The Jordan measure is an infimum of the finite sum of element measure. The Lebesgue measure is an infimum of the infinite sum of element measure.&lt;/p&gt;
&lt;p&gt;Is the Lebesgue outer measure (&lt;span class=&#34;math inline&#34;&gt;\(E^{*}\ = \ \inf \Sigma_{n=1}^{\infty} m(E_n)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(m(E)\)&lt;/span&gt; is elementary measure and &lt;span class=&#34;math inline&#34;&gt;\(A \subset \bigcup E_n\)&lt;/span&gt;), a &lt;strong&gt;limit&lt;/strong&gt; of Jordan measure (&lt;span class=&#34;math inline&#34;&gt;\(\lim\sup \Sigma_{n=1}^{N} m(E_n)\)&lt;/span&gt;)?&lt;/p&gt;
&lt;p&gt;Measure can be considered as a optimizaion problem.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
minimize \ \Sigma_{n=1}^{\infty} m(E_n) \\
suject \ to \ A \subset \bigcup E_n \\ 
where \ m(E) \ is \ elementary \ measure, \ E \ is \ a \ elementary \ set 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The objective function &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{n=1}^{\infty} m(E_n)\)&lt;/span&gt; has infinite &lt;strong&gt;domain&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f: E^{\infty} \to R\)&lt;/span&gt; in Lebesgue outer measure and finite &lt;strong&gt;domain&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f:E^{N} \to R\)&lt;/span&gt; in Jordan outer measure. The Lebesgue outer measure and Jordan outer measure has different domain space, then the objective function of the Lebesgue outer measure is not a limit of Jordan outer measure.&lt;/p&gt;
&lt;p&gt;Optimization problem has solution (&lt;span class=&#34;math inline&#34;&gt;\(\{E:E_n \ , n \in \mathbb{N} \ or \ \infty \}\)&lt;/span&gt;) at the saddle point where meets the objective function and the constraint. &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; that we measure is a parameter of the constraints. Measure is find solution (&lt;span class=&#34;math inline&#34;&gt;\(\{E:E_n \ , n \in \mathbb{N} \ or \ \infty \}\)&lt;/span&gt;) with constraints with &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; what we measure. Then the solution can be computed by approximation or limit process.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Laplace transformation</title>
      <link>/post/laplace-transformation/</link>
      <pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/laplace-transformation/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The Fourier series represents a periodic function as a descrete vectors. The Fourier transformation turns a time domain non-periodic function into a frequency domain continuous function. The Fourier series and transformation change a single time base &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; into infinite frequency basis &lt;span class=&#34;math inline&#34;&gt;\(e^{inx}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(e^{iwx}\)&lt;/span&gt;. The function on infinite basis domain can be represented by a vector or a function of basis domain &lt;span class=&#34;math inline&#34;&gt;\(v_{n}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(f(w)\)&lt;/span&gt;. This is a coefficients of Fourier series or Fourier transformation.&lt;/p&gt;
&lt;p&gt;The basis of Fourier transformation is pure frequency &lt;span class=&#34;math inline&#34;&gt;\(e^{iw}\)&lt;/span&gt;. The domain of Laplace transfomation is frequency &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; and damping component &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; which compose damping ocilation function, &lt;span class=&#34;math inline&#34;&gt;\(e^{s} = e^{(iw+\sigma)}\)&lt;/span&gt;. The function which represent Laplace transformation &lt;span class=&#34;math inline&#34;&gt;\(F(s)\)&lt;/span&gt; is a function of complex domain &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. The Fourier transformation is a special Laplace transformation of no damping term &lt;span class=&#34;math inline&#34;&gt;\(s = 0 \cdot \sigma +iw\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;periodic&lt;/strong&gt; function can be represented by a series not a continuous function. A condition makes a function can be represented by pure frequency domain i.e. Fourier transformation, not a complex domain i.e. Laplace transformation. The condition is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;from wikipedia &lt;a href=&#34;https://en.wikipedia.org/wiki/Laplace_transform#Fourier_transform&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Laplace_transform#Fourier_transform&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;math&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \widehat{f}(\omega) &amp;amp;= \mathcal{F}\{f(t)\} \\[4pt]
                  &amp;amp;= \mathcal{L}\{f(t)\}|_{s = i\omega}  =  F(s)|_{s = i \omega} \\[4pt]
                  &amp;amp;= \int_{-\infty}^\infty e^{-i \omega t} f(t)\,dt~.
\end{align}\]&lt;/span&gt;&lt;/math&gt;&lt;/p&gt;
&lt;p&gt;Laplace transformation makes a differential equation to an algebra equation.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Laplace transformation\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L}[f(t)] = F(s) = \int_{t=0}^{\infty} f(t)e^{-st}dt
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Transfer function\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H(s) = Y(s)/X(s)
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Y(s) = H(s)X(s)  
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X(s)\)&lt;/span&gt; are Laplace transformed &lt;span class=&#34;math inline&#34;&gt;\(y(t)\)&lt;/span&gt;, i.e. solution and &lt;span class=&#34;math inline&#34;&gt;\(f(t)\)&lt;/span&gt; i.e. input.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; which represents coefficients of damped frquency basis &lt;span class=&#34;math inline&#34;&gt;\(e^{\sigma + iw}\)&lt;/span&gt;. We are not looking for the solution &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt;. We are looking for the inverse Laplace transformation of &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt;. The inverse Laplace transformation turns a function &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt; with infinite damped frquency basis &lt;span class=&#34;math inline&#34;&gt;\(e^{\sigma + iw}\)&lt;/span&gt; to the solution of linear differential equation &lt;span class=&#34;math inline&#34;&gt;\(y(t)\)&lt;/span&gt; that is a function with a single domain basis &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Laplace transformation has poles that blow up at a point. The poles were determined by constants of differential equation and the input term.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Use Korean font in pdf generated from rmarkdown on Windows</title>
      <link>/post/2020-10-27-use-korean-font-in-rmarkdown/</link>
      <pubDate>Tue, 27 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-10-27-use-korean-font-in-rmarkdown/</guid>
      <description>


&lt;div id=&#34;yaml&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;yaml&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;---
mainfont: NanumGothic
output:
  pdf_document: 
    latex_engine: xelatex
---&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The mainfont is the LaTex option. The argument is a font family. The &lt;strong&gt;exact names&lt;/strong&gt; of the font family can be found in the fonts folder. Expand the header of the fonts folder. &lt;strong&gt;NanumGothic&lt;/strong&gt; is the exact name of the font family.&lt;/p&gt;
&lt;p&gt;The font family should be &lt;strong&gt;Editable embedding&lt;/strong&gt; type (&lt;a href=&#34;https://docs.microsoft.com/en-us/typography/opentype/spec/os2#fstype&#34; class=&#34;uri&#34;&gt;https://docs.microsoft.com/en-us/typography/opentype/spec/os2#fstype&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The LaTex engine should be xelatex.&lt;/p&gt;
&lt;p&gt;Let’s use another Korean font on pdf generated by rmarkdown.&lt;/p&gt;
&lt;p&gt;You can use Roboto google font with&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;header-includes:
    - \usepackage[sfdefault]{roboto}
    - \usepackage[T1]{fontenc}
output: 
  pdf_document: 
    latex_engine: pdflatex
mainfont: roboto&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Convolution and Fourier transformation</title>
      <link>/post/convolution-and-fourier-transformation/</link>
      <pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/convolution-and-fourier-transformation/</guid>
      <description>


&lt;p&gt;Convolution is a vector operation on two vectors.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Convolution \\ c * d = d*c \\ (c*d)_n = \Sigma_{i+j} c_i d_j = \Sigma_i c_i d_{n-i}.\]&lt;/span&gt;
This is multiplying polynomials. The parameters of multiplied polynomial become convolution of two polynomials. Fourier transformation expands x base to infinite exponential basis &lt;span class=&#34;math inline&#34;&gt;\(e^{iwk}\)&lt;/span&gt;. The &lt;strong&gt;multiplication on x (time) space&lt;/strong&gt; becomes &lt;strong&gt;convolutionn on k (frequency) space&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If time space is periodic, its Fourier transformation is discrete i.e. Fourier series. If time space is non-periodic, its Fourier transformation is continuous Fourier transformation.&lt;/p&gt;
&lt;p&gt;The Fourier transformation is dual. The relations of &lt;strong&gt;multiplication and convolution&lt;/strong&gt; and &lt;strong&gt;periodic and discrete&lt;/strong&gt; are dual in time space and frequency space.&lt;/p&gt;
&lt;p&gt;Fourier transformation is changing basis. The changing basis can be done by inner product (for vector space) or integration (function space) with new basis in which are we want move to space. This is why Fourier transformation coefficients calculated by integration with function multiplying basis &lt;span class=&#34;math inline&#34;&gt;\(e^{iwk}\)&lt;/span&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lagrange dual problem and conjugate function</title>
      <link>/post/lagrange-dual-problem-and-conjugate-function/</link>
      <pubDate>Sun, 13 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/lagrange-dual-problem-and-conjugate-function/</guid>
      <description>


&lt;p&gt;The optimization problem have two components that are objective function &lt;span class=&#34;math inline&#34;&gt;\(f_0 : \mathbb R ^n \rightarrow \mathbb R\)&lt;/span&gt; and the constraints. The objective function and constraints keep in check each other and make balance at saddle point i.e. optimal point. The dual (Lagrange) problem of the optimal problem also solve the optimization problem by making low boundary.&lt;/p&gt;
&lt;p&gt;The dual problem can be explained as a conjugate function &lt;span class=&#34;math inline&#34;&gt;\(f^* = \sup (x^Ty-f(x))\)&lt;/span&gt;. The Lagrangian is &lt;span class=&#34;math inline&#34;&gt;\(L(x, \lambda, \nu) = f_0(x) + \lambda f_1, + \nu f_2\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f_0\)&lt;/span&gt; is the objective function, &lt;span class=&#34;math inline&#34;&gt;\(f_1\)&lt;/span&gt; is inequality constraints and &lt;span class=&#34;math inline&#34;&gt;\(f_2\)&lt;/span&gt; is equality constraints. The Lagrangian function is &lt;span class=&#34;math inline&#34;&gt;\(g(\lambda,nu) = \inf_{x}L(x, \lambda, \nu) = \inf_{x}(f_0(x) + \lambda f_{1} + \nu f_{2})\)&lt;/span&gt;. The second and third term of the Lagrangian function is can be rewriten as an inner product form &lt;span class=&#34;math inline&#34;&gt;\(x^{T}h(\lambda) + x^{T}i(\nu)\)&lt;/span&gt; and constant term with &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. Then the inner product term &lt;span class=&#34;math inline&#34;&gt;\(x^{T}h(\lambda) + x^{T}i(\nu)\)&lt;/span&gt; and objective term becomes a conjugate function.&lt;/p&gt;
&lt;p&gt;The conjugate function &lt;span class=&#34;math inline&#34;&gt;\(f^*(x)\)&lt;/span&gt; is similar in terms of balance and saddle point.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Approximation</title>
      <link>/post/approximation/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/approximation/</guid>
      <description>


&lt;p&gt;The purpose of approximation is finding optimal point &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; i.e. &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x^*) = 0\)&lt;/span&gt;. We need a step/search direction &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt; and step size &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Taylor approximation has polynomial arguments that is a step and parameters of derivatives at the start point. The first degree of Taylor approximation has one adding term from start point &lt;span class=&#34;math inline&#34;&gt;\((x_0, F(x_0))\)&lt;/span&gt;. The adding term &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x) \Delta x\)&lt;/span&gt; is consistent with a parameter (gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x)\)&lt;/span&gt;) and a argument (step &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt;). The Taylor approximation does approximate &lt;span class=&#34;math inline&#34;&gt;\(F(x + \Delta x)\)&lt;/span&gt; for any search direction &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt;. We want to choose &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt; for the direction to the optimal point.&lt;/p&gt;
&lt;p&gt;The adding term of Taylor approximation &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x) \Delta x\)&lt;/span&gt; have level curve (level line). The smallest Euclidean norm of the level curve is achieved at the tangent. The gradient descent set the step to the gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x)\)&lt;/span&gt;. This makes the adding term biggest with Euclidean norm &lt;span class=&#34;math inline&#34;&gt;\(\Vert \nabla F(x) \Vert^2\)&lt;/span&gt; i.e. dual norm &lt;span class=&#34;math inline&#34;&gt;\(\Vert \nabla F(x) \Vert_*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Newton’s method is second degree of Taylor approximation &lt;span class=&#34;math inline&#34;&gt;\(F(x_0+\Delta x) \approx F(x_0) + \nabla F(x) \Delta x + 1/2\Delta x^T H \Delta x\)&lt;/span&gt;. We want to find &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt; to minimize the second degree of Taylor approximation. In this case, the minimizing step is tangent of first adding term &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x) \Delta x\)&lt;/span&gt; and second adding term &lt;span class=&#34;math inline&#34;&gt;\(\Delta x^T H \Delta x\)&lt;/span&gt; i.e. Steepest descent in H norm &lt;span class=&#34;math inline&#34;&gt;\(\Vert \cdot \Vert _H\)&lt;/span&gt;. The newton’s method can be thought as approximation of gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x_0 + \Delta x) \approx \nabla F(x_0) + H \Delta x = 0,\ \Delta x = -H^{-1} \nabla F(x_0)\)&lt;/span&gt;. This is also the derivative of second degree of Taylor approximation with respect to &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But the Taylor approximation is local. In addition to a step, a step size is needed. A step size determines how far the step taken. Backtracking line search has two constant parameters 0 &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; &amp;lt; 0.5, 0 &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &amp;lt; 1. The approximation is below the convex function. &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; tilts the slope i.e. gradient upside and the tilted approximation meets the convex function. &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the update rate of the step size until the the amount of the step is less than the point that tilted approximation meeets the convex function.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Singular vector decomposition</title>
      <link>/post/singular-vector-decomposition/</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/singular-vector-decomposition/</guid>
      <description>


&lt;p&gt;Bases are the central idea of linear algebra. An invertable square matrix has eigenvectors. A symetric matrix has orthogonal eigenvectors with non-negative eigenvalues, i.e. positive semidefinite. A matrix has two types of singular vectors, left and right signular vectors, &lt;span class=&#34;math inline&#34;&gt;\(A=U\Sigma V^{T}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;When we think the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is data points of rows &lt;span class=&#34;math inline&#34;&gt;\(A=U\Sigma V^{T}\)&lt;/span&gt; like data table, The right singular vectors &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; build bases, the sigular values &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; are magnitude of the bases and the left singular values &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; becomes new data points on new bases. The new data points &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; are orthonormal.&lt;/p&gt;
&lt;p&gt;When we think the matrix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is a system of linear transformation &lt;span class=&#34;math inline&#34;&gt;\(Ax=b,\ U\Sigma V^{T}x=b\)&lt;/span&gt;, a vector &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is repositioned on right singular vector coordinates &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; then the coordinates are multiplied by &lt;span class=&#34;math inline&#34;&gt;\(\Sigma\)&lt;/span&gt; and finally linear transformed by left singular vector &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A matrix is sum of rank one singular matrix. &lt;span class=&#34;math display&#34;&gt;\[A = \sigma_{1} u_{1}u_{1}^{T} + \cdots +  \sigma_{k} u_{k}u_{k}^{T}\]&lt;/span&gt; The Eckart-Young theorem finds closest low-rank matrix &lt;span class=&#34;math inline&#34;&gt;\(A_k\)&lt;/span&gt;.&lt;br /&gt;
In symetric matrix, the bases (right singular vectors) and it’s value on the bases (left singular vectors) are same. Reproducing kernel hilbert space has same values on it’s base functions.&lt;/p&gt;
&lt;p&gt;Rayleigh quotient $R(x) = {{x^{T}Sx} } $ has maximum &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{1}\)&lt;/span&gt; at the eigen vector &lt;span class=&#34;math inline&#34;&gt;\(q_{1}\)&lt;/span&gt; and saddle points at &lt;span class=&#34;math inline&#34;&gt;\(x=q_{k},\ \frac{\partial R}{\partial x_{i}} = 0\)&lt;/span&gt;. The second eigenvector can be found by Lagrangian optimization problum maximizing &lt;span class=&#34;math inline&#34;&gt;\(\ R(x)\)&lt;/span&gt; s.t. &lt;span class=&#34;math inline&#34;&gt;\(q_{1} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Pseudoinversion &lt;span class=&#34;math inline&#34;&gt;\(A^{+}\)&lt;/span&gt; process does first inversing value with &lt;span class=&#34;math inline&#34;&gt;\(U^{T}\)&lt;/span&gt;, and scale with &lt;span class=&#34;math inline&#34;&gt;\(\Sigma ^{+}\)&lt;/span&gt; and followed by reversing axis &lt;span class=&#34;math inline&#34;&gt;\(V^{T}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;When &lt;span class=&#34;math inline&#34;&gt;\(Ax=b\)&lt;/span&gt; has many solutions, minimizing &lt;span class=&#34;math inline&#34;&gt;\(\lVert A \rVert\)&lt;/span&gt; s.t. &lt;span class=&#34;math inline&#34;&gt;\(Ax=b\)&lt;/span&gt; can be best solution. The &lt;span class=&#34;math inline&#34;&gt;\(L_{1}\)&lt;/span&gt; norm has sparse solution.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Low rank matrix and compressed sensing</title>
      <link>/post/low-rank-matrix-and-compressed-sensing/</link>
      <pubDate>Tue, 14 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/post/low-rank-matrix-and-compressed-sensing/</guid>
      <description>


&lt;p&gt;This is a note for part III of Linear Algebra and learning from data, Gilbert Strang&lt;/p&gt;
&lt;p&gt;The main themes are sparsity (Low rank), Information theory (compression), and of course linear transformation.&lt;/p&gt;
&lt;p&gt;A full rank matrix is inefficient. Finding low lank matrix which is close with original matrix can save computation.&lt;/p&gt;
&lt;p&gt;The rank one matrix &lt;span class=&#34;math inline&#34;&gt;\(uv^{T}\)&lt;/span&gt; is a unit of a matrix. The full rank matrix can be decomposed by sum of rank one matrices i.e. singular vector decomposition.&lt;/p&gt;
&lt;p&gt;Sherman–Morrison formula suggests update rule for adding rank one matrix to the original matrix.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[(A + \mathbf{u} \mathbf{v}^{T})^{-1} = A^{-1} - \frac{A^{-1} \mathbf{u} \mathbf{v}^{T}A^{-1}}{1 + \mathbf{v}^{T} A^{-1} \mathbf{u}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The matrix norm is associated with singular value, &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The point is the unit of matrix is the rank one matrix, specially outer product of singular vectors &lt;span class=&#34;math inline&#34;&gt;\(uv^{T}\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(uv^{T}\)&lt;/span&gt; is a coordinate of the matrix space and singular value &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is a point on the coordinate.&lt;/p&gt;
&lt;p&gt;System, Inner product, &lt;span class=&#34;math inline&#34;&gt;\(A^{T}A\)&lt;/span&gt;, Steady state equilibrium, dual&lt;/p&gt;
&lt;p&gt;A system has a law. The observations follow the law. The state set by the system’s law. The state has two variables.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Ax=b \;(1)\\ Y= \beta X  \; (2)\\ \hat{x} = \frac {A^{T}b}{(A^{T}A) \; (3)\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Equation (1) is observations, (2) is a system, (3) is fit the observations to the system. &lt;span class=&#34;math inline&#34;&gt;\(A^{T}A\)&lt;/span&gt; is a steady state equilibrium. &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A^{T}\)&lt;/span&gt; are dual. &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; are dual.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Steady state equilibrium</title>
      <link>/post/steady-state-equilibrium/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/steady-state-equilibrium/</guid>
      <description>


&lt;p&gt;The meaning of &lt;span class=&#34;math inline&#34;&gt;\(A^{T}\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Steady state equilibrium&lt;/li&gt;
&lt;li&gt;Graph Laplacian matrix &lt;span class=&#34;math inline&#34;&gt;\(A^{T}CA\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Differential equation and Laplacian matrix&lt;/li&gt;
&lt;li&gt;Derivative is a graph without branch.&lt;/li&gt;
&lt;li&gt;Row space and column space are dual.&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A^{T}\)&lt;/span&gt; are dual.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;ref) Linear algebra and learning from data, Part IV, Gilbert Strang&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Differential equations and Fourier transformation</title>
      <link>/post/differential-equations-and-fourier-transformation/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/differential-equations-and-fourier-transformation/</guid>
      <description>


&lt;p&gt;Differential equations describe the change of state. The change relates to the state. The solutions of the differential equations are the status equations. The initial conditions set the time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and status &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The boundary conditions are the value of boundary &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(dy \over dt\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(= ay + q(t)\)&lt;/span&gt; starting from &lt;span class=&#34;math inline&#34;&gt;\(y(0)\)&lt;/span&gt; at $t=0. inital conditions &lt;span class=&#34;math inline&#34;&gt;\(t = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y=1\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(q(t)\)&lt;/span&gt; is a input and &lt;span class=&#34;math inline&#34;&gt;\(y(t)\)&lt;/span&gt; is a response. If &lt;span class=&#34;math inline&#34;&gt;\(q(t)\)&lt;/span&gt; is delta function, the response is said &lt;strong&gt;Impulse response&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[y&amp;#39; -ay = \delta (t) \\ y(t)=e^{at}\]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The solutions are combination of particular solution and null solution &lt;span class=&#34;math inline&#34;&gt;\(y = y_t + y_n\)&lt;/span&gt;. The solution includes &lt;span class=&#34;math inline&#34;&gt;\(e^{at}\)&lt;/span&gt;. The differential equations can not be solved like polynomial equations, because the arguments of the differentia equation relate to each other by calculus in the background of the equation. They can not be treated as just different arguments. The &lt;strong&gt;Fourier transformation&lt;/strong&gt; puts the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and its derivative &lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;\)&lt;/span&gt; in the same functional space (Hilbert space). This transformation makes the differential equation problem to simple arithmetic problem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fourier transformation &lt;span class=&#34;math inline&#34;&gt;\(F(x) = \Sigma ^{\infty}_{n=-\infty} c_{n}e^{inx}\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The basis of the Fourier transformation is &lt;span class=&#34;math inline&#34;&gt;\(e^{inx}\)&lt;/span&gt;. If the coefficients of the basis &lt;span class=&#34;math inline&#34;&gt;\(c_{n}\)&lt;/span&gt; decay fater, &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; becomes smooth. If the coefficients are constant, &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; is delta function &lt;span class=&#34;math inline&#34;&gt;\(\delta(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The derivative &lt;span class=&#34;math inline&#34;&gt;\(dy \over dt\)&lt;/span&gt; is an linear transformation operator, i.e. inner product, because the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;\)&lt;/span&gt; are in functional space with same basis. The defivative can be represented as a matix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. The derivative matrix is antisymetric i.e. &lt;span class=&#34;math inline&#34;&gt;\(A^T = -A\)&lt;/span&gt; and the minus second derivative matrix &lt;span class=&#34;math inline&#34;&gt;\(-d^{2}/dx^{2}\)&lt;/span&gt; is symetic positive definite. &lt;span class=&#34;math inline&#34;&gt;\(AAf = -A^{T}Af\)&lt;/span&gt;. The meaning of transverse of a matrix is &lt;span class=&#34;math inline&#34;&gt;\((Ax)^{T}y = x^{T}(A^{T}y)\)&lt;/span&gt;. &lt;strong&gt;Dual and inner product&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/secondDifferenceMat.png&#34; alt=&#34;Second differnce matrix K&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Second differnce matrix K&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The second difference matrix solves discrete differential equations. The N eigenvectors of K are &lt;span class=&#34;math inline&#34;&gt;\(y_{n} = (sin\ n\pi \Delta x, sin\ 2n\pi \Delta x,\ ..., sin\ Nn\pi \Delta x)\)&lt;/span&gt;. The N eigen values of K are the positive numbers &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{n} = 2-2cos {n \pi \over N+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;How does exponent &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; mean in &lt;span class=&#34;math inline&#34;&gt;\(e^i\)&lt;/span&gt;? The exponent makes multiplication to addition. What does an imaginary exponent mean? The imaginary exponent tilts the value to a complex plane. If the base is natural base &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, the value of &lt;span class=&#34;math inline&#34;&gt;\(e^i\)&lt;/span&gt; is in the unit circle of a complex plane. The cycle is &lt;span class=&#34;math inline&#34;&gt;\(2 \pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Fourier transformation for solving the difference equation provoked the subject of functional analysis 200 years ago.&lt;/p&gt;
&lt;p&gt;Reference&lt;br /&gt;
Differential Equations and Linear Algebra, Gilbert Strang&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information</title>
      <link>/post/information/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/information/</guid>
      <description>


&lt;p&gt;Information relates to uncertainty. The Shannon information content of an outcome &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(h(x)=-log_{2}P(x)\)&lt;/span&gt;. The rare event has larger information than a common event. The unit of information is a bit (binary digit). Coding is a mapping from an outcome of an ensemble to binary digits &lt;span class=&#34;math inline&#34;&gt;\(\{0,1\}^+\)&lt;/span&gt;. A symbol code is a code for a &lt;strong&gt;single&lt;/strong&gt; ensemble. A block code is a code for a &lt;strong&gt;sequence&lt;/strong&gt; ensemble. A set of sequences of the ensemble has a typical subset. The cardinality of a typical set is &lt;span class=&#34;math inline&#34;&gt;\(2^{H_{2}X}\)&lt;/span&gt;. We can reduce a code length by mapping codes to only a typical set (the source coding theorem). The prefix code is an optimal symbol code. The Kraft inequality is the condition of prefix code &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{i}2^{-l_{i}} \le 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The noisy-channel coding theorem describes the possible rate and block code length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. If the block code length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is long enough, the channel looks like the noisy typewriter and arbitrary block error rate can be achieved with rate. The maximum rate is the capacity &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; of the channel. If the rate is small enough, the typical set of the output of the channel can be mapped for the typical set of input without overlap.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing pulications of HUGO academic blog in Rstudio</title>
      <link>/post/managing-pulications-of-hugo-academic-blog-in-rstudio/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      <guid>/post/managing-pulications-of-hugo-academic-blog-in-rstudio/</guid>
      <description>


&lt;p&gt;This HUGO academic theme blog is managed by blogdown. (&lt;a href=&#34;https://bookdown.org/yihui/blogdown/other-themes.html&#34; class=&#34;uri&#34;&gt;https://bookdown.org/yihui/blogdown/other-themes.html&lt;/a&gt;) The HUGO theme needs to transform &lt;code&gt;bib&lt;/code&gt; file to each &lt;code&gt;reference.md&lt;/code&gt; files. This can be done by &lt;code&gt;bib2academic&lt;/code&gt; rpackage. (&lt;a href=&#34;https://github.com/petzi53/bib2academic&#34; class=&#34;uri&#34;&gt;https://github.com/petzi53/bib2academic&lt;/a&gt;) For the featured publication, add &lt;code&gt;featured: true&lt;/code&gt; in the &lt;code&gt;reference.md&lt;/code&gt; file.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Taylor series</title>
      <link>/post/taylor-series/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/taylor-series/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x) = \sum_{k=0}^\infty c_k x^k = c_0 + c_1 x + c_2 x^2 + \dotsb. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is an approximation that is a function of h and derivatives of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; are elements of parameters.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x \pm h) = f(x) \pm hf&amp;#39;(x) + \frac{h^2}{2}f&amp;#39;&amp;#39;(x) \pm \frac{h^3}{6}f&amp;#39;&amp;#39;&amp;#39;(x) + O(h^4)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s think about &lt;span class=&#34;math inline&#34;&gt;\(\sin(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x) = \sin(x) \ f(0) = 0, f&amp;#39;(x)=\cos(x)\ f&amp;#39;(0)=1, f&amp;#39;&amp;#39;(x)=-\sin(x)\ f&amp;#39;&amp;#39;(0)=0 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*} \sin(x) &amp;amp;= 0 + \frac{1}{1!}x + \frac{0}{2!}x^2 + \frac{-1}{3!}x^3 + \dotsb
&amp;amp;= x - \frac{x^3}{3!} + \frac{x^5}{5!} - \dotsb, \end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is approximation. Now &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; and parameters calculated from derivatives of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;.&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x \pm h) = f(x) \pm hf&amp;#39;(x) + \frac{h^2}{2}f&amp;#39;&amp;#39;(x) \pm \frac{h^3}{6}f&amp;#39;&amp;#39;&amp;#39;(x) + O(h^4)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/sine-better-models.png&#34; alt=&#34;https://betterexplained.com/articles/taylor-series/&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;a href=&#34;https://betterexplained.com/articles/taylor-series/&#34; class=&#34;uri&#34;&gt;https://betterexplained.com/articles/taylor-series/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Taylor series and Newton’s bionomial theorem explain the complex exponent.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\exp(z) = e^{z}, \ z = a+bi \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The imaginary exponent is hard to understand intuitively. The exponential function &lt;span class=&#34;math inline&#34;&gt;\(e^{x}\)&lt;/span&gt; on a complex domain can be regarded as a function exp(x) that behaves like exponential function, i.e. a product of functions is addion of arguments &lt;span class=&#34;math inline&#34;&gt;\(\exp(x) \exp(y) = \exp(x+y)\)&lt;/span&gt;. The product of &lt;span class=&#34;math inline&#34;&gt;\(\exp\)&lt;/span&gt; fucntion becomes addition of arguments by Newton’s binomical theorem. The costomary expression is &lt;span class=&#34;math inline&#34;&gt;\(e^{x}\)&lt;/span&gt;. This can be done when &lt;span class=&#34;math inline&#34;&gt;\(\exp(x) = \Sigma ^{\infty}_{n=0} \frac {Z^{n}}{n!}\)&lt;/span&gt; The taylor series with repidly decaying pactorial coefficients &lt;span class=&#34;math inline&#34;&gt;\(n!\)&lt;/span&gt;. This series converges absolutely for every complex &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and converges uniformly on every bounded subset of the complex plain. Rudin’s Real and complex analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My first github issue</title>
      <link>/post/my-first-github-issue/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/my-first-github-issue/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://github.com/tidymodels/recipes/issues/482&#34; class=&#34;uri&#34;&gt;https://github.com/tidymodels/recipes/issues/482&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodel and glmnet</title>
      <link>/post/tidymodel-and-glmnet/</link>
      <pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/tidymodel-and-glmnet/</guid>
      <description>


&lt;p&gt;When the penalized generalize linear model (Lasso or Ridge) is processed in the tidymodel environment, finalizing the hyperparameter (lambda) and getting coefficients of the final model are confusing. Here is an example. This example predicts PIK3CA mutation status by gene expression data. TCGA breast cancer dataset is used.&lt;/p&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Modeling&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(glmnet)
library(themis)

set.seed(930093)
cv_splits &amp;lt;- rsample::vfold_cv(trainset_ahDiff, strata = PIK3CA_T)
mod &amp;lt;- logistic_reg(penalty = tune(),
                    mixture = tune()) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)

rec &amp;lt;- recipe(PIK3CA_T ~ ., data = trainset_ahDiff) %&amp;gt;%
  step_BoxCox(all_numeric()) %&amp;gt;%
  step_dummy(HISTOLOGICAL_DIAGNOSIS) %&amp;gt;%
  step_center(all_numeric()) %&amp;gt;%
  step_scale(all_numeric()) %&amp;gt;%
  step_smote(PIK3CA_T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wfl &amp;lt;- workflow() %&amp;gt;%
  add_recipe(rec) %&amp;gt;%
  add_model(mod)

glmn_set &amp;lt;- parameters(penalty(range = c(-5,1), trans = log10_trans()),
                       mixture())

glmn_grid &amp;lt;- 
  grid_regular(glmn_set, levels = c(7, 5))
ctrl &amp;lt;- control_grid(save_pred = TRUE, verbose = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Grid parameter search on 10-fold cross-validation with 5 repeats&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Dummy variable to control for histologic subtype&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;select-best-parameter&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Select best parameter&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glmn_tune &amp;lt;- 
  tune_grid(wfl,
            resamples = cv_splits,
            grid = glmn_grid,
            metrics = metric_set(roc_auc),
            control = ctrl)


best_glmn &amp;lt;- select_best(glmn_tune, metric = &amp;quot;roc_auc&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;finalizing&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Finalizing&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wfl_final &amp;lt;- 
  wfl %&amp;gt;%
  finalize_workflow(best_glmn) %&amp;gt;%
  fit(data = trainset_ahDiff)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;finalize_workflow()&lt;/code&gt; finalizes the model with selected optimal hyperparameters. However, the glmnet fits any lambda, not the indicated lambda. This was discussed at &lt;a href=&#34;https://github.com/tidymodels/parsnip/issues/195&#34; class=&#34;uri&#34;&gt;https://github.com/tidymodels/parsnip/issues/195&lt;/a&gt;. The glmnet is more efficient to fit all lambda than a single lambda. Thus tidymodel ignores the indicated lambda. This made the first confusion. &lt;strong&gt;The finalization can be finalized by predict in tidymodel environment.&lt;/strong&gt; Finalize with &lt;code&gt;predict&lt;/code&gt;. Note the last argument &lt;code&gt;penalty = 1&lt;/code&gt; of &lt;code&gt;stats::predict(wfl_final, type = &#34;prob&#34;, new_data = trainset_ahDiff, penalty = 1)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_predict &amp;lt;- stats::predict(wfl_final, type = &amp;quot;prob&amp;quot;, new_data = trainset_ahDiff, penalty = 1)
train_probs &amp;lt;- 
  predict(wfl_final, type = &amp;quot;prob&amp;quot;, new_data = trainset_ahDiff) %&amp;gt;%
  bind_cols(obs = trainset_ahDiff$PIK3CA_T) %&amp;gt;%
  bind_cols(predict(wfl_final, new_data = trainset_ahDiff))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Performance&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat(train_probs, obs, .pred_class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Truth
## Prediction Wild Mutant
##     Wild    213     45
##     Mutant  123    158&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(roc_curve(train_probs, obs, .pred_Mutant, event_level = &amp;quot;second&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-18-tidymodel-and-glmnet_files/figure-html/performance-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roc_auc(train_probs, obs, .pred_Mutant, event_level = &amp;quot;second&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 roc_auc binary         0.770&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because glmnet fits the whole path, there are whole coefficients in the glmnet fit object &lt;code&gt;wfl_final&lt;/code&gt;. This was the second confusion. How to get the final model coefficients is below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficients&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Coefficients&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(extract_model(wfl_final)) %&amp;gt;%
  filter(lambda &amp;gt; 0.98 &amp;amp; lambda &amp;lt; 1.01)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17 x 5
##    term                                           step estimate lambda dev.ratio
##    &amp;lt;chr&amp;gt;                                         &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 (Intercept)                                      55  -0.0630   1.00     0.123
##  2 C4A                                              55   0.0587   1.00     0.123
##  3 C5orf13                                          55   0.0587   1.00     0.123
##  4 CDSN                                             55   0.0706   1.00     0.123
##  5 CFB                                              55   0.0719   1.00     0.123
##  6 CYP21A2                                          55   0.0516   1.00     0.123
##  7 DGKE                                             55  -0.0709   1.00     0.123
##  8 FGD5                                             55   0.0670   1.00     0.123
##  9 GALNT10                                          55   0.0575   1.00     0.123
## 10 GOLM1                                            55   0.0689   1.00     0.123
## 11 GPX8                                             55   0.0657   1.00     0.123
## 12 KLK11                                            55   0.0145   1.00     0.123
## 13 NTN4                                             55   0.0578   1.00     0.123
## 14 SMYD3                                            55   0.0637   1.00     0.123
## 15 USP36                                            55  -0.0698   1.00     0.123
## 16 WBP2                                             55  -0.0652   1.00     0.123
## 17 HISTOLOGICAL_DIAGNOSIS_Infiltrating.Lobular.~    55  -0.0244   1.00     0.123&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>/post/entropy/</link>
      <pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/entropy/</guid>
      <description>


&lt;p&gt;This is a note for Elements of information theory of Thomas M. Cover.&lt;/p&gt;
&lt;p&gt;The entropy (&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;) is a measure of uncertainty of a variable which is the answer to what is the ultimate data compression. Is the conditional probability &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt; considered as a probability of the “conditional variable” &lt;span class=&#34;math inline&#34;&gt;\((X|Y=y)\)&lt;/span&gt;? Yes, it is the subset of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt;. If you sum all of the subset probabilities, it becomes the cardinality of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Thus if you make that become 1, the conditional probability should be multiplied with &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;. The conditional probability is larger than joint probabilities &lt;span class=&#34;math inline&#34;&gt;\(p(x,y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s think about the entropy of a joint random variable &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are correlated, the entries of the contingent table of &lt;span class=&#34;math inline&#34;&gt;\(p(x,y)\)&lt;/span&gt; are concentrated at some points that mean lager or smaller probabilities than a product of &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;. The joint probability is a product of probability of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and conditional probability &lt;span class=&#34;math inline&#34;&gt;\(p(X|Y)\)&lt;/span&gt;. The conditional entropy is the expectation of a random conditional variable (conditional entropy). The conditional entropy does not mean the entropy of a subset of &lt;span class=&#34;math inline&#34;&gt;\(X|Y=y\)&lt;/span&gt;. It is a measure of uncertainty of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; has the &lt;strong&gt;information&lt;/strong&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, the entropy of &lt;span class=&#34;math inline&#34;&gt;\(X|Y\)&lt;/span&gt; is less than &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The conditional entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X|Y)\)&lt;/span&gt; is subtract the entropy of Y &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; from joint entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X,Y)\)&lt;/span&gt;. The joint probability &lt;span class=&#34;math inline&#34;&gt;\(p(x,y)\)&lt;/span&gt; is the product of probability of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and conditional probability &lt;span class=&#34;math inline&#34;&gt;\(p(x|Y=y)\)&lt;/span&gt;. This is chain rule of joint entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X,Y) = H(Y) + H(X|Y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The chain rule is converting a joint variable to the sum of conditional random variables. The joint variable is the sum of conditional random variables. This is can be applied in the entropy, the information, and the relative entropy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Duality</title>
      <link>/post/duality/</link>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/duality/</guid>
      <description>


&lt;p&gt;This is summary of Boyd convex optimization. Steepest descent method is a convex optimization algorithm. The normalized steepest descent direction &lt;span class=&#34;math inline&#34;&gt;\(x_{nsd}\)&lt;/span&gt; is a vector of unit ball of a norm that extends in the direction &lt;span class=&#34;math inline&#34;&gt;\(-\nabla f(x)\)&lt;/span&gt;. The inner product of &lt;span class=&#34;math inline&#34;&gt;\(x_{nsd}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(-\nabla f(x)\)&lt;/span&gt; is maximized. The first order Taylor approximation of &lt;span class=&#34;math inline&#34;&gt;\(f(x+v) = f(x) + \nabla f(x)^{T} v\)&lt;/span&gt; is most efficient when &lt;span class=&#34;math inline&#34;&gt;\(v = x_{nsd}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(x_{nsd}\)&lt;/span&gt; is unnormalized into &lt;span class=&#34;math inline&#34;&gt;\(x_{sd}\)&lt;/span&gt;. The normalization is ralated with unit ball of norm. When &lt;span class=&#34;math inline&#34;&gt;\(x_{nsd}\)&lt;/span&gt; is scaled with dual norm of &lt;span class=&#34;math inline&#34;&gt;\(-\nabla f(x)\)&lt;/span&gt;, the second term of Taylor approximation &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(x)^{T} x_{sd}\)&lt;/span&gt; becomes convex (squre of &lt;strong&gt;dual norm&lt;/strong&gt; of gradient of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;). The unnormalized &lt;span class=&#34;math inline&#34;&gt;\(x_{sd}\)&lt;/span&gt; the amount of movement of approximation because the inner product of gradient of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; and unnormalized steepest descent direction is squre of &lt;strong&gt;dual norm&lt;/strong&gt; of gradient.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;dual norm&lt;/strong&gt; of gradient &lt;span class=&#34;math inline&#34;&gt;\(\lVert \nabla f(x) \rVert\)&lt;/span&gt; is main subject of this post. The simplest dual is a complement of a set. The &lt;span class=&#34;math inline&#34;&gt;\((C^c)^c\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is small, &lt;span class=&#34;math inline&#34;&gt;\(C^C\)&lt;/span&gt; is large and vice versa. The dual cone is related to inner product and non-negativity. Let &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; be a cone, The set &lt;span class=&#34;math inline&#34;&gt;\(K^{*} = \{y|x^{T}y \geq 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(x \in K\}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is large, &lt;span class=&#34;math inline&#34;&gt;\(K^{*}\)&lt;/span&gt; is small and vice versa.&lt;/p&gt;
&lt;p&gt;The dual norm &lt;span class=&#34;math inline&#34;&gt;\(\left\lVert x \right\rVert _{*}\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\sup \{\, x^{T}y \mid \lVert y \rVert \leq 1 \,\}\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; direction is long axis of ellypsoid norm, the norm of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is small. But the dual norm is large because &lt;span class=&#34;math inline&#34;&gt;\(\lVert y \rVert _{2}\)&lt;/span&gt; large and vice versa.&lt;/p&gt;
&lt;p&gt;The main points are the first order Taylor approximation is most efficient with ellypsoid norm when the linear approximation is &lt;span class=&#34;math inline&#34;&gt;\(\sup\{\nabla f(x) ^{T} x_{sd}\}\)&lt;/span&gt; which is &lt;strong&gt;dual norm&lt;/strong&gt; of gradient of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Strong convexity and implications</title>
      <link>/post/strong-convexity-and-implications/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/strong-convexity-and-implications/</guid>
      <description>


&lt;p&gt;This is a summary of the Boyd convex optimization book. The strong convexity assumption can be useful to explain the iterative minimization algorithms like gradient descent, steepest descent, and Newton’s method.&lt;/p&gt;
&lt;p&gt;The smallest and largest eigen value of Hessian &lt;span class=&#34;math inline&#34;&gt;\(m \preceq \nabla^{2}f(x) \preceq M\)&lt;/span&gt; with norm of gradient &lt;span class=&#34;math inline&#34;&gt;\(\| \nabla f(x)\|_2\)&lt;/span&gt; determine the boundary of optimal value &lt;span class=&#34;math inline&#34;&gt;\(p^{*}\)&lt;/span&gt;. The condition number of &lt;strong&gt;cond&lt;/strong&gt;(&lt;span class=&#34;math inline&#34;&gt;\(C_\alpha\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\leq {M \over m}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(C_\alpha\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;-sublevel. The condition number is ratio of largest eigen value to its smallest eigen value.&lt;/p&gt;
&lt;p&gt;When the Hessian is replaced with a constant &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, the constants make the equality of Taylor’s theorem to inequality that makes lower and upper boundaries of &lt;span class=&#34;math inline&#34;&gt;\(p^*\)&lt;/span&gt;. The difference between the approximation and &lt;span class=&#34;math inline&#34;&gt;\(p^*\)&lt;/span&gt; is determined by $ f(x)$ and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(x)\)&lt;/span&gt; is small and the gap is so. If Hessian (&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;) is large, the gap is small.&lt;/p&gt;
&lt;p&gt;Because the second degree of Tayler’s expansion is quadratic, at near the optimal point, the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;-sublevel is ellipsoid. The condition number &lt;strong&gt;cond&lt;/strong&gt;(&lt;span class=&#34;math inline&#34;&gt;\(C_{\alpha}\)&lt;/span&gt;), geometrically, represents anisometry or eccentricity&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodel</title>
      <link>/post/tidymodel/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/tidymodel/</guid>
      <description>


&lt;div id=&#34;machine-learning-and-tidymodel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Machine Learning and Tidymodel&lt;/h3&gt;
&lt;div id=&#34;model-setting-parsnip&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Model setting, {Parsnip}&lt;/h4&gt;
&lt;p&gt;Rpackage Parsnip standardizes model specification. Tidymodel follows the concept of lazy evaluation of the tidyverse. Parsnip sets unified specifications and lately evaluates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering-recipes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Feature engineering, {Recipes}&lt;/h4&gt;
&lt;p&gt;Recipes make preprocessing easy with &lt;code&gt;step_()&lt;/code&gt; functions. Recipes after specification calculate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resampling-rsample&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Resampling, {rsample}&lt;/h4&gt;
&lt;p&gt;To choose a model and hyperparameters, we must validate the different models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;making-hyperparameter-set-dials&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Making hyperparameter set, {dials}&lt;/h4&gt;
&lt;p&gt;The Rpackage {dials} set hyperparameter similarily with {Parsnip}. {Dials} standadize parameter of each modeling algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-modeling-process-workflowr&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Set modeling process, {Workflowr}&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-models-with-hyperparameter-tunes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit models with hyperparameter, {tunes}&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>RStudio Conference 2020</title>
      <link>/post/rstudio-conference-2020/</link>
      <pubDate>Wed, 29 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/rstudio-conference-2020/</guid>
      <description>


&lt;p&gt;Key Note&lt;/p&gt;
&lt;p&gt;J.J. Allaire&lt;/p&gt;
&lt;p&gt;RStudio becomes Public B Corp.&lt;/p&gt;
&lt;p&gt;J.J. Allaire’s favorite book Fooled by Randomness&lt;/p&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;RStudio has restructured as a &amp;#39;benefit corporation,&amp;#39; legally allowing it to consider the needs of its users and the &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; community and not just its shareholders, JJ Allaire announced just now at &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;a href=&#34;https://t.co/uG6SjNeLei&#34;&gt;https://t.co/uG6SjNeLei&lt;/a&gt;&lt;/p&gt;&amp;mdash; Sharon Machlis (@sharon000) &lt;a href=&#34;https://twitter.com/sharon000/status/1222577749824401408?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/Rstudio?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#Rstudio&lt;/a&gt; evolution &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;a href=&#34;https://t.co/euZFBTpvVY&#34;&gt;pic.twitter.com/euZFBTpvVY&lt;/a&gt;&lt;/p&gt;&amp;mdash; 1LittleCoder💻 (@1littlecoder) &lt;a href=&#34;https://twitter.com/1littlecoder/status/1222574214747607042?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/h2&gt;
&lt;p&gt;Google AI PAIR team&lt;br /&gt;
Fernanda Viegas, Martin Wattenberg&lt;/p&gt;
&lt;p&gt;Debug your data first, not your program!&lt;/p&gt;
&lt;p&gt;Visualization, Hello World Image data set has faulty annotations.&lt;br /&gt;
Validate your data set!!&lt;/p&gt;
&lt;p&gt;High dimensionality, MNIST data set PCA clustering.&lt;/p&gt;
&lt;p&gt;Fairness, Different views of points can be visualized. People can understand it.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Great to see misclassification applied to fairness in loans! &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstudio?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudio&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/google?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#google&lt;/a&gt; &lt;a href=&#34;https://t.co/0TOCCegOpt&#34;&gt;pic.twitter.com/0TOCCegOpt&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kyle Monahan (@kylemonahan2) &lt;a href=&#34;https://twitter.com/kylemonahan2/status/1222585238473203712?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;If you are doing data science, you are doing user experience design - with great power comes great responsibility &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;br&gt;&lt;br&gt;Loved this keynote! &lt;a href=&#34;https://twitter.com/wattenberg?ref_src=twsrc%5Etfw&#34;&gt;@wattenberg&lt;/a&gt; &lt;a href=&#34;https://twitter.com/viegasf?ref_src=twsrc%5Etfw&#34;&gt;@viegasf&lt;/a&gt; &lt;a href=&#34;https://t.co/goNzLkhSUe&#34;&gt;pic.twitter.com/goNzLkhSUe&lt;/a&gt;&lt;/p&gt;&amp;mdash; Alison Presmanes Hill (@apreshill) &lt;a href=&#34;https://twitter.com/apreshill/status/1222593520508141568?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Next big &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; keynote quote that I love:&lt;br&gt;&lt;br&gt;Debug your data first, not your program!&lt;/p&gt;&amp;mdash; Hunter Glanz (@hglanz) &lt;a href=&#34;https://twitter.com/hglanz/status/1222581456829992960?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;a href=&#34;https://pair.withgoogle.com&#34; class=&#34;uri&#34;&gt;https://pair.withgoogle.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Andrew Mangan, Salesforce&lt;/p&gt;
&lt;p&gt;Analytics using R&lt;/p&gt;
&lt;p&gt;Mine Cetinkaya-Rundelm, RStudio&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;shiny-contest-open-codes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Shiny contest, open codes&lt;/h2&gt;
&lt;p&gt;Joe Cheng, RStudio&lt;/p&gt;
&lt;p&gt;Styling Shiny with Sass and Bootstrap 4&lt;br /&gt;
Bootstrap 4 is default Shiny frame. Bootstrap 4 supports Sass which is a meta-program for CSS. {bootstraplib} makes easy to use Sass in R environment.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Thanks to &lt;a href=&#34;https://twitter.com/jcheng?ref_src=twsrc%5Etfw&#34;&gt;@jcheng&lt;/a&gt; for the shoutout at &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt;! Be sure to check out &lt;a href=&#34;https://twitter.com/jcheng?ref_src=twsrc%5Etfw&#34;&gt;@jcheng&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/cpsievert?ref_src=twsrc%5Etfw&#34;&gt;@cpsievert&lt;/a&gt;’s package bootstraplib for easy custom fonts and styling in &lt;a href=&#34;https://twitter.com/hashtag/shiny?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#shiny&lt;/a&gt; &lt;a href=&#34;https://t.co/hcy5zCxuAQ&#34;&gt;https://t.co/hcy5zCxuAQ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Tim Mastny (@timmastny) &lt;a href=&#34;https://twitter.com/timmastny/status/1222664917280387072?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;A nice resource for learning shiny from &lt;a href=&#34;https://twitter.com/_ColinFay?ref_src=twsrc%5Etfw&#34;&gt;@_ColinFay&lt;/a&gt;:&lt;a href=&#34;https://t.co/RtKU6sowqj&#34;&gt;https://t.co/RtKU6sowqj&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt;&lt;/p&gt;&amp;mdash; John Blischak (@jdblischak) &lt;a href=&#34;https://twitter.com/jdblischak/status/1222652894354141184?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;/div&gt;
&lt;div class=&#34;section level2&#34;&gt;
&lt;h2&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;What if we kissed 😳😳 in &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; Grand Ballroom B 🙈 &lt;a href=&#34;https://t.co/UXYxH6mjWQ&#34;&gt;pic.twitter.com/UXYxH6mjWQ&lt;/a&gt;&lt;/p&gt;&amp;mdash; Dr. Jacqueline Nolis (@skyetetra) &lt;a href=&#34;https://twitter.com/skyetetra/status/1222603023605624832?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;.&lt;a href=&#34;https://twitter.com/skyetetra?ref_src=twsrc%5Etfw&#34;&gt;@skyetetra&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/heatherklus?ref_src=twsrc%5Etfw&#34;&gt;@heatherklus&lt;/a&gt; have released the OSS loadtest package! Run and analyze load tests from entirely within R&lt;br&gt;&lt;br&gt;Learn more: &lt;a href=&#34;https://t.co/3Xq8J9jqfA&#34;&gt;https://t.co/3Xq8J9jqfA&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf&lt;/a&gt; &lt;a href=&#34;https://t.co/LwkwO4DDP1&#34;&gt;pic.twitter.com/LwkwO4DDP1&lt;/a&gt;&lt;/p&gt;&amp;mdash; David Robinson (@drob) &lt;a href=&#34;https://twitter.com/drob/status/1222611273021394944?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Standing room only for &lt;a href=&#34;https://twitter.com/paleolimbot?ref_src=twsrc%5Etfw&#34;&gt;@paleolimbot&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; talk on programming with ggplot2! Did you know that you can put plot items into a list and add that?? &lt;a href=&#34;https://t.co/wGMEPSBIeP&#34;&gt;pic.twitter.com/wGMEPSBIeP&lt;/a&gt;&lt;/p&gt;&amp;mdash; Calum You (@_calumyou) &lt;a href=&#34;https://twitter.com/_calumyou/status/1222991783933575168?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Just learned that adding NULL to a ggplot does nothing...which is super useful when writing functions that wrap ggplot. ggplot(...) + NULL is the same as ggplot(...). Thanks &lt;a href=&#34;https://twitter.com/paleolimbot?ref_src=twsrc%5Etfw&#34;&gt;@paleolimbot&lt;/a&gt;! &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt;&lt;/p&gt;&amp;mdash; Taylor Reiter (@ReiterTaylor) &lt;a href=&#34;https://twitter.com/ReiterTaylor/status/1222991203110551554?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/h2&gt;
&lt;p&gt;Carson Sievert, RStudio&lt;/p&gt;
&lt;p&gt;Reproducible Shiny, Download tab, {shinymeta}&lt;/p&gt;
&lt;p&gt;Aymen Waqar&lt;/p&gt;
&lt;p&gt;Building a native iPad dashboard&lt;/p&gt;
&lt;p&gt;Ana Alyeska Santos, Braulio Cuandon, Biosense Webster, Inc.&lt;/p&gt;
&lt;p&gt;Reproducible Engineering Test Reports&lt;/p&gt;
&lt;p&gt;Justin Juskewitch, Mayo Clinic&lt;br /&gt;
Transfusion platelet selection and match for patient and documentation automation&lt;/p&gt;
&lt;p&gt;Alicia Schep, Outlier AI&lt;/p&gt;
&lt;p&gt;She moved R from Python. {vlBuildr} API develop Altair and Vega-Lite&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;Tyson Barrett&lt;/p&gt;
&lt;p&gt;List-columns in data.table&lt;/p&gt;
&lt;p&gt;data.table can easily convert list-columns&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Learn how to use list-columns with data.table from these slides from &lt;a href=&#34;https://twitter.com/healthandstats?ref_src=twsrc%5Etfw&#34;&gt;@healthandstats&lt;/a&gt;:&lt;a href=&#34;https://t.co/amXG686twc&#34;&gt;https://t.co/amXG686twc&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;a href=&#34;https://t.co/CrPVDhtYgH&#34;&gt;https://t.co/CrPVDhtYgH&lt;/a&gt;&lt;/p&gt;&amp;mdash; John Blischak (@jdblischak) &lt;a href=&#34;https://twitter.com/jdblischak/status/1223004598572437504?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Lightning Talk&lt;/p&gt;
&lt;p&gt;Mexican election prediction&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Really interesting talk from &lt;a href=&#34;https://twitter.com/TeresaOM?ref_src=twsrc%5Etfw&#34;&gt;@TeresaOM&lt;/a&gt; on predicting Mexican election results from initial polling data. They&amp;#39;re in a &amp;quot;bunker&amp;quot; with no internet access, so no StackOverflow!&lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt;&lt;a href=&#34;https://t.co/8Im6k6zusu&#34;&gt;https://t.co/8Im6k6zusu&lt;/a&gt;&lt;/p&gt;&amp;mdash; John Blischak (@jdblischak) &lt;a href=&#34;https://twitter.com/jdblischak/status/1223021112117743618?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Rebecca Barter Becoming an R Blogger&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Stupidly excited for &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; this week! &lt;br&gt;&lt;br&gt;📢 Check out my lightening talk on &amp;quot;becoming an R blogger&amp;quot; Thursday at 2:55. &lt;br&gt;&lt;br&gt;👋 If you&amp;#39;d like to say hi during the conference, feel free to reach out&lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/datascience?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#datascience&lt;/a&gt;&lt;/p&gt;&amp;mdash; Rebecca Barter (@rlbarter) &lt;a href=&#34;https://twitter.com/rlbarter/status/1221852227184128000?ref_src=twsrc%5Etfw&#34;&gt;January 27, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;Other sessions, not check-in&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;🎨📊 Enjoying learning about { ggtext } and { gridtext } &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; packages from &lt;a href=&#34;https://twitter.com/ClausWilke?ref_src=twsrc%5Etfw&#34;&gt;@ClausWilke&lt;/a&gt; @ &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt;&lt;a href=&#34;https://t.co/vyqgTkTnM8&#34;&gt;https://t.co/vyqgTkTnM8&lt;/a&gt;&lt;a href=&#34;https://t.co/jnnR0AvGF0&#34;&gt;https://t.co/jnnR0AvGF0&lt;/a&gt; &lt;a href=&#34;https://t.co/4ew8EKqXay&#34;&gt;pic.twitter.com/4ew8EKqXay&lt;/a&gt;&lt;/p&gt;&amp;mdash; Rich Pauloo, PhD (@RichPauloo) &lt;a href=&#34;https://twitter.com/RichPauloo/status/1222998335239290880?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Session: &amp;quot;Updates on Spark, MLFlow &amp;amp; the Broader ML Ecosystem&amp;quot; (including Delta Lake)&lt;br&gt;&lt;br&gt;Use Spark in R via Sparklyr ✨&lt;a href=&#34;https://twitter.com/hashtag/rstudioconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;a href=&#34;https://t.co/j1ITayLf8G&#34;&gt;pic.twitter.com/j1ITayLf8G&lt;/a&gt;&lt;/p&gt;&amp;mdash; Daniel ModeratelyOnline McNichol (@dnlmc) &lt;a href=&#34;https://twitter.com/dnlmc/status/1222651913352577024?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Sage advice for putting R models in production from &lt;a href=&#34;https://twitter.com/heatherklus?ref_src=twsrc%5Etfw&#34;&gt;@heatherklus&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/skyetetra?ref_src=twsrc%5Etfw&#34;&gt;@skyetetra&lt;/a&gt;:&lt;br&gt;&lt;br&gt;- Avoid too many tests by only testing the most critical behavior&lt;br&gt;- Load test to find bottlenecks&lt;br&gt;- Give people a tool to explore and understand the model&lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt;&lt;/p&gt;&amp;mdash; John Blischak (@jdblischak) &lt;a href=&#34;https://twitter.com/jdblischak/status/1222612815694856193?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Realtime Spell Check and global replacement in &lt;a href=&#34;https://twitter.com/rstudio?ref_src=twsrc%5Etfw&#34;&gt;@rstudio&lt;/a&gt; IDE 1.3!! &lt;br&gt;&lt;br&gt;🥳🙌&lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf&lt;/a&gt; &lt;a href=&#34;https://t.co/6cICDvWbdq&#34;&gt;pic.twitter.com/6cICDvWbdq&lt;/a&gt;&lt;/p&gt;&amp;mdash; annakrystalli (@annakrystalli) &lt;a href=&#34;https://twitter.com/annakrystalli/status/1222965028229271558?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Enjoying &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; .  Here are the slides from the talk I gave yesterday: &lt;a href=&#34;https://t.co/R7vxqENMN3&#34;&gt;https://t.co/R7vxqENMN3&lt;/a&gt;&lt;/p&gt;&amp;mdash; Frank Harrell (@f2harrell) &lt;a href=&#34;https://twitter.com/f2harrell/status/1222931681570504704?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;How to build native iPad app using R and plumber under the hood - &lt;a href=&#34;https://twitter.com/D_Rodziewicz?ref_src=twsrc%5Etfw&#34;&gt;@D_Rodziewicz&lt;/a&gt; &lt;a href=&#34;https://twitter.com/appsilon?ref_src=twsrc%5Etfw&#34;&gt;@appsilon&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rshiny?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rshiny&lt;/a&gt; &lt;a href=&#34;https://t.co/tShpA0gc7h&#34;&gt;pic.twitter.com/tShpA0gc7h&lt;/a&gt;&lt;/p&gt;&amp;mdash; Paweł Przytuła (@pawel_appsilon) &lt;a href=&#34;https://twitter.com/pawel_appsilon/status/1222679720455618560?ref_src=twsrc%5Etfw&#34;&gt;January 30, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Great message during the talk of &lt;a href=&#34;https://twitter.com/MT_statistics?ref_src=twsrc%5Etfw&#34;&gt;@MT_statistics&lt;/a&gt; at the &lt;a href=&#34;https://twitter.com/RLadiesGlobal?ref_src=twsrc%5Etfw&#34;&gt;@RLadiesGlobal&lt;/a&gt; breakfast &lt;a href=&#34;https://twitter.com/hashtag/rstudioconf2020?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf2020&lt;/a&gt; &lt;a href=&#34;https://t.co/XWevhIFP4N&#34;&gt;pic.twitter.com/XWevhIFP4N&lt;/a&gt;&lt;/p&gt;&amp;mdash; Joselyn Chavez (@josschavezf1) &lt;a href=&#34;https://twitter.com/josschavezf1/status/1222559524839542784?ref_src=twsrc%5Etfw&#34;&gt;January 29, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;Some presentation highlights: (1) &lt;a href=&#34;https://twitter.com/W_R_Chase?ref_src=twsrc%5Etfw&#34;&gt;@W_R_Chase&lt;/a&gt; on graph styling: &lt;a href=&#34;https://t.co/JVgUrhPxLm&#34;&gt;https://t.co/JVgUrhPxLm&lt;/a&gt; (2) &lt;a href=&#34;https://twitter.com/ClausWilke?ref_src=twsrc%5Etfw&#34;&gt;@ClausWilke&lt;/a&gt; on formatting ggplot2 text (so now you can implement some of &lt;a href=&#34;https://twitter.com/W_R_Chase?ref_src=twsrc%5Etfw&#34;&gt;@W_R_Chase&lt;/a&gt;&amp;#39;s suggestions): &lt;a href=&#34;https://t.co/rQX89X45dM&#34;&gt;https://t.co/rQX89X45dM&lt;/a&gt; (3) &lt;a href=&#34;https://twitter.com/JennyBryan?ref_src=twsrc%5Etfw&#34;&gt;@JennyBryan&lt;/a&gt; on debugging in R: &lt;a href=&#34;https://t.co/nTg2D6SCSN&#34;&gt;https://t.co/nTg2D6SCSN&lt;/a&gt; [2/n]&lt;/p&gt;&amp;mdash; Alex Albright (@AllbriteAllday) &lt;a href=&#34;https://twitter.com/AllbriteAllday/status/1223317400054988801?ref_src=twsrc%5Etfw&#34;&gt;January 31, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;@fellgernon&lt;/span&gt; kindly tweeted me.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;&lt;a href=&#34;https://twitter.com/hashtag/rstudioconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf&lt;/a&gt; workshops day 1 lunch:&lt;br&gt;&lt;br&gt;Jun Kang is a clinical assistant professor at a hospital in South Korea who uses R for many things from exploring data with &lt;a href=&#34;https://twitter.com/hashtag/shiny?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#shiny&lt;/a&gt; to building predictive models&lt;a href=&#34;https://twitter.com/JKang1978?ref_src=twsrc%5Etfw&#34;&gt;@JKang1978&lt;/a&gt; &lt;a href=&#34;https://t.co/13rARrqbWs&#34;&gt;https://t.co/13rARrqbWs&lt;/a&gt; &lt;a href=&#34;https://t.co/oG59N82o2U&#34;&gt;https://t.co/oG59N82o2U&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; 3/11 &lt;a href=&#34;https://t.co/JFVTuWbrrg&#34;&gt;pic.twitter.com/JFVTuWbrrg&lt;/a&gt;&lt;/p&gt;&amp;mdash; 🇲🇽 Leonardo Collado-Torres (@lcolladotor) &lt;a href=&#34;https://twitter.com/lcolladotor/status/1224034257712173057?ref_src=twsrc%5Etfw&#34;&gt;February 2, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;On my latest blog post I talk about how empowering it can be when you help others make connections☺️. That is, becoming a sponsor as defined by &lt;a href=&#34;https://twitter.com/robinson_es?ref_src=twsrc%5Etfw&#34;&gt;@robinson_es&lt;/a&gt;. It&amp;#39;s worth it!&lt;br&gt;&lt;br&gt;Thx &lt;a href=&#34;https://twitter.com/StefanieButland?ref_src=twsrc%5Etfw&#34;&gt;@StefanieButland&lt;/a&gt; for the motivation to write this post!&lt;a href=&#34;https://t.co/PY0KCC5yAi&#34;&gt;https://t.co/PY0KCC5yAi&lt;/a&gt;&lt;a href=&#34;https://twitter.com/hashtag/rstudioconf?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstudioconf&lt;/a&gt; &lt;a href=&#34;https://twitter.com/hashtag/rstats?src=hash&amp;amp;ref_src=twsrc%5Etfw&#34;&gt;#rstats&lt;/a&gt; &lt;a href=&#34;https://t.co/MUkHvmXdBD&#34;&gt;pic.twitter.com/MUkHvmXdBD&lt;/a&gt;&lt;/p&gt;&amp;mdash; 🇲🇽 Leonardo Collado-Torres (@lcolladotor) &lt;a href=&#34;https://twitter.com/lcolladotor/status/1224561372236828672?ref_src=twsrc%5Etfw&#34;&gt;February 4, 2020&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;And he left some inspiring post.
&lt;a href=&#34;http://lcolladotor.github.io/2020/02/03/conference-feelings-from-newbie-to-sponsor/&#34; class=&#34;uri&#34;&gt;http://lcolladotor.github.io/2020/02/03/conference-feelings-from-newbie-to-sponsor/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applied Machine Learning Workshop RStudio Conference 2020</title>
      <link>/post/applied-machine-learning-workshop-rstudio-conference-2020/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/applied-machine-learning-workshop-rstudio-conference-2020/</guid>
      <description>


&lt;p&gt;This is a note of applied machine learning workshop RStudion conference 2020&lt;/p&gt;
&lt;p&gt;Why is it hard to predict (domain knowledge).&lt;/p&gt;
&lt;p&gt;purrr::map allows inline code.&lt;/p&gt;
&lt;p&gt;purrr::map and tidyr::nest covered because they are used in resample or tune.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-26-applied-machine-learning-workshop-rstudio-conference-2020_files/figure-html/explore-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Skew data might be looking outlier.&lt;/p&gt;
&lt;p&gt;People look at data in many different ways like outliers, missingness, correlation, and suspicion of an important variable.&lt;/p&gt;
&lt;p&gt;The ggplot is good to explore variables adding geoms changing plot.&lt;/p&gt;
&lt;p&gt;Machine learning is creative because you can do many different ways such as which variable should be included.&lt;/p&gt;
&lt;p&gt;Model workflow: imputation -&amp;gt; transformation -&amp;gt; filter -&amp;gt; model&lt;/p&gt;
&lt;p&gt;Resampling avoids something like the garden of forking paths or p-hacking by honest feedback.&lt;/p&gt;
&lt;p&gt;The tuning parameter can be estimated &lt;strong&gt;analytically&lt;/strong&gt; or iteratively.&lt;/p&gt;
&lt;p&gt;Single validation set ??&lt;/p&gt;
&lt;p&gt;Resampling process questions&lt;/p&gt;
&lt;p&gt;Splitting data is confusing.&lt;/p&gt;
&lt;p&gt;The test set split preserves &lt;strong&gt;distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Interval and sampling are behind the scene of resampling.&lt;/p&gt;
&lt;p&gt;fomular x ~ . -something to remove&lt;/p&gt;
&lt;p&gt;Variable role: limited (can not be applied to hierarchical in Baysian??)&lt;/p&gt;
&lt;p&gt;Formula and XY interface is not fit in machine learning. The recipes package is a solution.&lt;/p&gt;
&lt;p&gt;The broom::glance (one-row summary of metrics, don’t trust this too much) tidy (coefficients) augment (by data points)&lt;/p&gt;
&lt;p&gt;The parsnip (unified interface) is a solution to a different interface.&lt;/p&gt;
&lt;p&gt;How to set specific settings using parsnip?&lt;/p&gt;
&lt;p&gt;Many other packages supporting parsnip. Look at the tidymodels packages.&lt;/p&gt;
&lt;p&gt;What is the difference between caret and parsnip?&lt;/p&gt;
&lt;p&gt;Caret~base R, parsnip~tidyverse&lt;/p&gt;
&lt;p&gt;The parsnip generalizes the interfaces and is easy to extend.&lt;/p&gt;
&lt;p&gt;A dummy variable is confusing in the interaction of terms and how to interpret coefficiency.&lt;/p&gt;
&lt;p&gt;The level with 0,0 allocation becomes a base variable in factor variable. A continuous variable can be categorized with recipe::step_discritize().&lt;/p&gt;
&lt;p&gt;A zero variance predictor includes no record or single value variable.&lt;/p&gt;
&lt;p&gt;Feature engineering problem
* Dummy variable&lt;br /&gt;
* Zero variance
* Standardize&lt;/p&gt;
&lt;p&gt;Customizing Step function&lt;/p&gt;
&lt;p&gt;The first level is a reference. There are ways to change the reference level. (Ordering factor level issues)&lt;/p&gt;
&lt;p&gt;People respond positively to the recipes.&lt;/p&gt;
&lt;p&gt;recipe -&amp;gt; prep -&amp;gt; juice/bake; define -&amp;gt; calculate -&amp;gt; processing (training or test set)&lt;/p&gt;
&lt;p&gt;The post-processed data set goes to fit() to establish a model&lt;/p&gt;
&lt;p&gt;Versioning model caching?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_rec &amp;lt;- recipe(
  Sale_Price ~ Longitude + Latitude + Neighborhood, 
  data = ames_train
) %&amp;gt;%
  step_log(Sale_Price, base = 10) %&amp;gt;%
  
  # Lump factor levels that occur in 
  # &amp;lt;= 5% of data as &amp;quot;other&amp;quot;
  step_other(Neighborhood, threshold = 0.05) %&amp;gt;%
  
  # Create dummy variables for _any_ factor variables
  step_dummy(all_nominal()) %&amp;gt;%
  step_nzv(
    starts_with (&amp;quot;Neighborhood_&amp;quot;))

preped_data &amp;lt;- prep(mod_rec) 

preped_data %&amp;gt;%
  juice() %&amp;gt;%
  slice(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 11
##   Longitude Latitude Sale_Price Neighborhood_Co… Neighborhood_Ol…
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1     -93.6     42.1       5.24                0                0
## 2     -93.6     42.1       5.39                0                0
## 3     -93.6     42.1       5.28                0                0
## 4     -93.6     42.1       5.29                0                0
## 5     -93.6     42.1       5.33                0                0
## # … with 6 more variables: Neighborhood_Edwards &amp;lt;dbl&amp;gt;,
## #   Neighborhood_Somerset &amp;lt;dbl&amp;gt;, Neighborhood_Northridge_Heights &amp;lt;dbl&amp;gt;,
## #   Neighborhood_Gilbert &amp;lt;dbl&amp;gt;, Neighborhood_Sawyer &amp;lt;dbl&amp;gt;,
## #   Neighborhood_other &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;step_dummy -&amp;gt; step_other/step_nzv()&lt;/p&gt;
&lt;p&gt;Step_dummy creates many dummy variables thus you should include the new dummy variables into the interaction terms or others or nzv. with starts_with() function.&lt;/p&gt;
&lt;p&gt;Resolving skewness: &lt;strong&gt;Box-cox&lt;/strong&gt;, &lt;strong&gt;inverse&lt;/strong&gt;, of course, log or square root&lt;/p&gt;
&lt;p&gt;parsnip model object includes original model results in &lt;code&gt;$fit&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The workflow replaces prep() -&amp;gt; juice() -&amp;gt; fit() with single call fit() and bake() -&amp;gt; predict() with predict ().&lt;/p&gt;
&lt;p&gt;The workflow needs &lt;code&gt;add&lt;/code&gt;ing model specification (parsnip) and preprocessing (recipes).&lt;/p&gt;
&lt;p&gt;Resampling is the best option to estimate the performance of a model.&lt;/p&gt;
&lt;p&gt;Resampling splits analysis set and assessment set on &lt;strong&gt;training set&lt;/strong&gt;. The final result is performance metrics.&lt;/p&gt;
&lt;p&gt;Selecting the resampling method relates to bias-variance trade-off. (Repeated CV)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Model_selection#Criteria&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Model_selection#Criteria&lt;/a&gt; information criteria??&lt;/p&gt;
&lt;p&gt;Resampling does stratified sampling.&lt;/p&gt;
&lt;p&gt;The tuning metric tends to be smooth. The irregular metric result means high variance.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm&#34; class=&#34;uri&#34;&gt;http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Resampling spends memory. The vfold_cv() has a copy of the original data.&lt;/p&gt;
&lt;p&gt;Processing first or resampling first??&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resample first&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If preprocessing is outside of resampling, You don’t know how the test set will be predicted.&lt;/p&gt;
&lt;p&gt;Is feature engineering stable or unstable?&lt;/p&gt;
&lt;p&gt;Can tune package select a good preprocessing process? Yes, it can.&lt;/p&gt;
&lt;p&gt;The upsampling issues and rare important data points&lt;/p&gt;
&lt;p&gt;The tuning hyperparameter (underfitting or overfitting).&lt;/p&gt;
&lt;p&gt;Hyperparameter types grid (regular, irregular), iterative&lt;/p&gt;
&lt;p&gt;Making function name: use underbar &lt;code&gt;_&lt;/code&gt; “Grid_regular”&lt;/p&gt;
&lt;p&gt;The pros and cons of regular and non-regular grid&lt;/p&gt;
&lt;p&gt;The non-regular grid efficient except some model can do trick.&lt;/p&gt;
&lt;p&gt;The parsnip standardizes and it changes default parameter ranges.&lt;/p&gt;
&lt;p&gt;The parameter tibble can not be subsetted. It will be issued in GitHub.&lt;/p&gt;
&lt;p&gt;People take care of the default value of parameters.&lt;/p&gt;
&lt;p&gt;Tidymodel delays evaluation. Set first and run later.&lt;/p&gt;
&lt;p&gt;The Splines fitting is wagged at the edge of the range. When trying to predict value out of range, warnings occur. Resampling can cause an error at the edge point of the assessment set.&lt;/p&gt;
&lt;p&gt;There is a note column in the result object.&lt;/p&gt;
&lt;p&gt;The sensitivity of parameters can be evaluated with ggplot. The scale should be adjusted when an outlier is present.&lt;/p&gt;
&lt;p&gt;Best fit can be choose&lt;/p&gt;
&lt;p&gt;Documentation&lt;/p&gt;
&lt;p&gt;Show best and select best (parameter)&lt;/p&gt;
&lt;p&gt;Tune tunes selected parameters. If you don’t tune parameters, the parameters go default value.&lt;/p&gt;
&lt;p&gt;The log ridership is wired because of a bimodal distribution. It will show skewed residual or something.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;!!stations&lt;/code&gt; indicates the environment of the station object, the station object is defined at recipe object, not the global environment.&lt;/p&gt;
&lt;p&gt;Regularization wins wrapping feature selection methods in terms of efficacy.&lt;/p&gt;
&lt;p&gt;If two predictors are highly correlated, the signs can change and the variance of coefficients inflates.&lt;/p&gt;
&lt;p&gt;L1 penalty does feature selection, the L2 penalty resolves correlation. Does mixture feature selection? Yes, the L1 component and lambda determine how many variables are kicked out, except pure ridge regression (ie. pure L2 penalty).&lt;/p&gt;
&lt;p&gt;Normalize or standardize on dummy variables?&lt;/p&gt;
&lt;p&gt;Lambda is a more important parameter than alpha in glm.&lt;/p&gt;
&lt;p&gt;Parallelism issue. GPU is good at linear algebra. Parallelism consumes memory because they copy data. Tuning is advantaged with parallelism because it uses for a loop. Unix is better than Windows in terms of parallelism.&lt;/p&gt;
&lt;p&gt;Inner join makes prediction tibble subsetting best.&lt;/p&gt;
&lt;p&gt;Repeated CV residual can be estimated with average predicting values.&lt;/p&gt;
&lt;p&gt;GCV is used to pruning the point.&lt;/p&gt;
&lt;p&gt;Is MARS greedy? Semi-greedy.&lt;/p&gt;
&lt;p&gt;MARS is struggling with colinearity. Use step_pca.&lt;/p&gt;
&lt;p&gt;Bayesian process, Gaussian process, Kernel function selection&lt;/p&gt;
&lt;p&gt;MARS hyperparameter space is high dimensional, thus the Gaussian process is better than the grid method in terms of efficiency. Grid methods for the Chicago data MARS model need more than 4000 points of the grid, although the grid search can use a parallel process.&lt;/p&gt;
&lt;p&gt;The Bayesian parameter searching process can be updated and pause.&lt;/p&gt;
&lt;p&gt;Classification Hard prediction vs soft prediction (probability)&lt;/p&gt;
&lt;p&gt;Accuracy can be estimated with hard prediction and ROC can be produced on soft prediction.&lt;/p&gt;
&lt;p&gt;Feature hashing is difficult to understand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;recipe can be tune.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;C5.0 is boost_tree.&lt;/p&gt;
&lt;p&gt;Gitter discussion
&lt;a href=&#34;https://gitter.im/conf2020-applied-ml/community/archives/2020/01/28&#34; class=&#34;uri&#34;&gt;https://gitter.im/conf2020-applied-ml/community/archives/2020/01/28&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convex set</title>
      <link>/post/convex-set/</link>
      <pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/convex-set/</guid>
      <description>


&lt;p&gt;There is a homology between a line segment and a convex set. It is helpful to understand the convex set. A line, a line segment, and one sideline has homology to an affine set, a convex set, and a cone.
A line is &lt;span class=&#34;math inline&#34;&gt;\(\{y|y=\theta_1 x_1 + \theta_2 x_2, \theta_1 + \theta_2 = 1\}\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(\theta_1, \theta_2 \in \mathbb{R}\)&lt;/span&gt;, a line segment is if &lt;span class=&#34;math inline&#34;&gt;\(\theta_1, \theta_2 &amp;gt; 0\)&lt;/span&gt; and an one side line if any &lt;span class=&#34;math inline&#34;&gt;\(\theta_1, \theta_2 &amp;lt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A set &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; is affine set if $ y C$ and &lt;span class=&#34;math inline&#34;&gt;\(\{y|y=\theta_1 x_1 + \theta_2 x_2, \theta_1 + \theta_2 = 1, x_1, x_2 \in C, \theta_1, \theta_2 \in \mathbb{R} \}\)&lt;/span&gt;. a convex set is if &lt;span class=&#34;math inline&#34;&gt;\(\theta_1, \theta_2 &amp;gt; 0\)&lt;/span&gt; and a cone if any &lt;span class=&#34;math inline&#34;&gt;\(\theta_1, \theta_2 &amp;lt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An affine set is a convex set. But all convex set is not an affine set. It looks the convex set has a stronger condition than affine set i.e. positivity of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. But in fact, the convex set has a stronger condition on what it should contain. Because an affine set contains more than a convex set, an affine set satisfies the condition to be a convex set.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>/post/reproducing-kernel-hilbert-space/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/reproducing-kernel-hilbert-space/</guid>
      <description>


&lt;p&gt;Finally arrive at reproducing kernel Hilbert space.
&lt;a href=&#34;https://nzer0.github.io/reproducing-kernel-hilbert-space.html&#34; class=&#34;uri&#34;&gt;https://nzer0.github.io/reproducing-kernel-hilbert-space.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The above post introduces RKHS in Korean. It was helpful. I had struggled to understand some concepts in RKHS. What does mean Hilbert space in terms of feature expansion? (&lt;span class=&#34;math inline&#34;&gt;\(f:\mathcal{X} \to \mathbb{R}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f \in \mathcal{H}_K\)&lt;/span&gt;) It was confusing the difference between &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; means the function in Hilbert space and &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is &lt;strong&gt;evaluation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I thought that the function can be represented by the inner product of the basis of feature space &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot,x)\)&lt;/span&gt; and coefficients &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, and the coefficients are vectors in feature space.&lt;/p&gt;
&lt;p&gt;The reproducing property of Kernel is &lt;span class=&#34;math inline&#34;&gt;\(\langle f, K(\cdot,x)\rangle_{\mathcal{H}} = f(x)\)&lt;/span&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot,x) \in \mathcal{H}_K\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot,x)\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; specified function in Hilbert space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_K\)&lt;/span&gt; and an evaluator of the specific point x. This means the inner product of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K_{x}\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a nutshell, kenel method is a &lt;strong&gt;different way of evaluating f in a specific point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/strong&gt;. &lt;strong&gt;Evaluating a function&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at a point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is inner product of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L_x\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(L_x \in \mathcal{H}_K\)&lt;/span&gt; is a &lt;strong&gt;evaluation functional&lt;/strong&gt; which is a kernal function and linear &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot, x)\)&lt;/span&gt;. Reproducing property of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_K\)&lt;/span&gt; can be achieved if all &lt;span class=&#34;math inline&#34;&gt;\(f \in \mathcal{H}\)&lt;/span&gt; has bounded evaluation functionals (&lt;span class=&#34;math inline&#34;&gt;\(L_x\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In least square methods, the parameters (&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;) are determined by inner product of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = (X^{T}X)^{-1}X^{T}y\)&lt;/span&gt;. In Kernel method, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; is determined &lt;span class=&#34;math inline&#34;&gt;\(\langle K(\cdot,x_i), K(\cdot,x_j), \rangle_{\mathcal{H}_K} = K(x_i, x_j)\)&lt;/span&gt;. Each &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot, x)\)&lt;/span&gt; is a parameter and a argument (variable like &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Some subclass of the loss function and penalty functions can be generated by a positive definite kernel. A Kernel accepts two arguments and a Kernel function does one argument and the other argument becomes parameter. Reproducing Kernel Hilbert space is a function space with Kernal function space with the evaluation functional as a Kernel. The feature expansion into the RKHS can use the Kernel matrix instead of the inner product of each variable &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The important concepts are Hilbert space, inner product, Kernel function, evaluation functional, feature expansion, Fourier transformation, Reisz representation theorem (dual space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_{K}^*\)&lt;/span&gt; of Hibert space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_K\)&lt;/span&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Asan surgical pathology update</title>
      <link>/post/asan-surgical-pathology-update/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/asan-surgical-pathology-update/</guid>
      <description>

&lt;h3 id=&#34;dr-dina-r-mody&#34;&gt;Dr. Dina R. Mody&lt;/h3&gt;

&lt;h4 id=&#34;hpv-test&#34;&gt;HPV test&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;DNA HPV test will fail.&lt;/li&gt;
&lt;li&gt;BD (Oncolarity) is for a screening test.&lt;/li&gt;
&lt;li&gt;genotype 66 high risk.&lt;/li&gt;
&lt;li&gt;False-negative 10%. DNA copy number is the cause.&lt;/li&gt;
&lt;li&gt;False positive cross reactivity 0.35%, 0.71% Aptima and Cobas. (one quarter)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-sneige&#34;&gt;Dr. Sneige&lt;/h3&gt;

&lt;h4 id=&#34;immunostain-in-breast&#34;&gt;Immunostain in breast&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Reduced and diminished myoepithelial cells, phenotypic alteration, SMA remain but others lost.&lt;/li&gt;
&lt;li&gt;Any positivity for MEC favors in situ than invasive.&lt;/li&gt;
&lt;li&gt;Metaplastic carcinoma and fibromatosis like&lt;/li&gt;
&lt;li&gt;TTF-1 2% positive in breast cancer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-prieto&#34;&gt;Dr. Prieto&lt;/h3&gt;

&lt;h4 id=&#34;cutaneous-lymphoma&#34;&gt;Cutaneous lymphoma&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Mycosis fungoides cell are atypical 7 fold folding nuclear membrane&lt;/li&gt;
&lt;li&gt;CD4 predominant than CD8&lt;/li&gt;
&lt;li&gt;Mycosis fungoides with large cell transformation&lt;/li&gt;
&lt;li&gt;CD8 mycosis fungoides&lt;/li&gt;
&lt;li&gt;Granulomatous MF&lt;/li&gt;
&lt;li&gt;Immunohistochemistry marker related to TX

&lt;ul&gt;
&lt;li&gt;CD25, CD30&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;again-dr-mody&#34;&gt;Again Dr. Mody&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Super D specimen preparation&lt;/li&gt;
&lt;li&gt;Guideline will be released.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-medeiros-hodgkin-lymphomas&#34;&gt;Dr. Medeiros, Hodgkin lymphomas&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Popcorn, not poped&lt;/li&gt;
&lt;li&gt;Hodgkin lymphoma cells were renamed.&lt;/li&gt;
&lt;li&gt;LP cells, Lacunar cells, RS + mononuclear variants/anaplastic variants&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-el-naggar-h-n-cancer&#34;&gt;Dr. El-Naggar H&amp;amp;N cancer&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;HPV nasopharyngeal carcinoma&lt;/li&gt;
&lt;li&gt;Young caucasian male&lt;/li&gt;
&lt;li&gt;H&amp;amp;N is different from anogenital area&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-moran&#34;&gt;Dr. Moran&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Neuroendocrine carcinoma in lung&lt;/li&gt;
&lt;li&gt;Do not follow the WHO without own justification&lt;/li&gt;
&lt;li&gt;Mitoses count is not rigorous measurement. eyepiece diameters, hotspot vs random&lt;/li&gt;
&lt;li&gt;Arrigoni criteria 1974 was 5-10 mitosis per 10 HPF&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-jihun-kim-colitis&#34;&gt;Dr. Jihun Kim colitis&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Yersinia&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Suppurative granulomas&lt;/li&gt;
&lt;li&gt;Heaped up mass-like mucosal thickening&lt;/li&gt;
&lt;li&gt;Superficial ileocecal lesion&lt;/li&gt;
&lt;li&gt;Lymphoid follicles&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;NSAID-induced enteropathy&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Radiating fibrosis around the ulcer&lt;/li&gt;
&lt;li&gt;Diaphragm-like stricture&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Nivolumab-induced colitis&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-moran-salivary-gland-type-tumors-of-the-lung&#34;&gt;Dr. Moran Salivary-gland type tumors of the lung&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Central intrabronchial mass&lt;/li&gt;
&lt;li&gt;Frozen diagnosis important sleeve or total pneumonectomy&lt;/li&gt;
&lt;li&gt;Adenosquamous is different with mucoepidermoid&lt;/li&gt;
&lt;li&gt;Mucoepidermoid m/c and children&lt;/li&gt;
&lt;li&gt;MAML2 translocation&lt;/li&gt;
&lt;li&gt;Acinic cell carcinoma gold standard is EM&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-sneige-breast-cancer-biomarkers&#34;&gt;Dr. Sneige Breast cancer biomarkers&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;ER test SP1 Scandle &lt;a href=&#34;https://www.cbc.ca/news/canada/newfoundland-labrador/lab-mistakes-poor-oversight-flagged-in-n-l-breast-cancer-inquiry-1.793504&#34; target=&#34;_blank&#34;&gt;https://www.cbc.ca/news/canada/newfoundland-labrador/lab-mistakes-poor-oversight-flagged-in-n-l-breast-cancer-inquiry-1.793504&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fixation time 6hr - 72 hr&lt;/li&gt;
&lt;li&gt;False negative&lt;/li&gt;
&lt;li&gt;Control tissues&lt;/li&gt;
&lt;li&gt;HER2 test 2018 guidelines&lt;/li&gt;
&lt;li&gt;OncotypeDX NEJM 2018 intermediated group can skip chemotherapy&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-prieto-cutaneous-mesenchymal-tumors&#34;&gt;Dr. Prieto Cutaneous mesenchymal tumors&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Dermatofibroma vs Dermatofibrosarcoma protuberance&lt;/li&gt;
&lt;li&gt;Epithelioid fibrous histiocytoma

&lt;ul&gt;
&lt;li&gt;Resembling melanoma&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Angiomatoid fibrous histiocytoma

&lt;ul&gt;
&lt;li&gt;Translocation EWSR1, FUS, CREB1, ATF1&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Atypical fibroxanthoma

&lt;ul&gt;
&lt;li&gt;Solitary lesion on sun-exposed skin&lt;/li&gt;
&lt;li&gt;Elderly adult&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Neurothekeoma&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-medeiros-marginal-zone-lymphomas&#34;&gt;Dr. Medeiros Marginal zone lymphomas&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;There category vs two cathegory (feat Dr. In-sun Kim)&lt;/li&gt;
&lt;li&gt;Colonization in follicles&lt;/li&gt;
&lt;li&gt;Cause of disease infection

&lt;ul&gt;
&lt;li&gt;H. pylori&lt;/li&gt;
&lt;li&gt;HCV&lt;/li&gt;
&lt;li&gt;Campylobacter jejuni&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Boring immunophenotype, important translocations

&lt;ul&gt;
&lt;li&gt;API2 and MALT1&lt;/li&gt;
&lt;li&gt;IGH and MALT1&lt;/li&gt;
&lt;li&gt;FOXP1 and MALT1&lt;/li&gt;
&lt;li&gt;BCL10 and IGH&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Large cell transformaion&lt;/li&gt;
&lt;li&gt;Splenic marginal zone lymphoma

&lt;ul&gt;
&lt;li&gt;Systemic involvement, bone marrow, peripheral blood&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-el-naggar&#34;&gt;Dr. El-Naggar&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Salivary gland tumor

&lt;ul&gt;
&lt;li&gt;Ductal carcinoma&lt;/li&gt;
&lt;li&gt;Homology with breast, prostate&lt;/li&gt;
&lt;li&gt;Testosterone, Androgen, and Steroid&lt;/li&gt;
&lt;li&gt;Androgen receptor, Glucocorticoid receptor&lt;/li&gt;
&lt;li&gt;Androgen receptor splicing variant: constitutive activation&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;dr-kyu-rae-kim&#34;&gt;Dr. Kyu-Rae Kim&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Progestin treatment failure

&lt;ul&gt;
&lt;li&gt;Myometrial invasion, basal layer involvement&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;No guideline for residual tumor&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ion2cbioportal Rpackage</title>
      <link>/post/ion2cbioportal-rpackage/</link>
      <pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/ion2cbioportal-rpackage/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://github.com/Jkang-alien/ion2cbioportal&#34; class=&#34;uri&#34;&gt;https://github.com/Jkang-alien/ion2cbioportal&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is Rpackage for making the cbioportal dataset from the ion torrent NGS result files. The formats of ion torrent result files are vcf and tsv. Those files convert to maf format and other data set for cbioportal. The package includes vignette. &lt;code&gt;vignette(&amp;quot;ion2cbioportal&amp;quot;)&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Limit of inequality of sequence and epsilon</title>
      <link>/post/limit-of-inequality-and-epsilon/</link>
      <pubDate>Sat, 09 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/post/limit-of-inequality-and-epsilon/</guid>
      <description>


&lt;p&gt;Here I summarize some tools for proof of the Riesz representation theorem. They are the limit of inequality of sequence and &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. The Rudin’s proof of the Riesz representation theorem construct measure &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and measurable set &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}\)&lt;/span&gt;, then prove the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}\)&lt;/span&gt; have properties. Countable additivity (not subadditivity) is an important property. The strategy of proving equality (additivity) is bidirectional inequality.&lt;/p&gt;
&lt;p&gt;Limit of inequality of sequence gives us a tool that finite inequality makes infinite inequality. &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; changes left and right parts of inequality (bidirectional inequality).&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^n_1\mu(K) \le \Sigma^n_1\mu(V_i)\)&lt;/span&gt; then &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^{\infty}_1 \mu(K) \le \Sigma^{\infty}_1 \mu(V)\)&lt;/span&gt;, K is compact and V is open. We can change both sides of inequality with &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\Sigma^{\infty}_1 \mu(V) \le \Sigma^{\infty}_1 \mu(K) + \epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Urison’s lemma is used for &lt;span class=&#34;math inline&#34;&gt;\(\mu(E) = \inf\mu(V) = \sup\mu(K)\)&lt;/span&gt; if $ E  $ and $ K E V$.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Positive Borel measures</title>
      <link>/post/positive-borel-measures/</link>
      <pubDate>Wed, 16 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/positive-borel-measures/</guid>
      <description>


&lt;p&gt;This is a note of real and complex analysis chapter 2.&lt;/p&gt;
&lt;p&gt;Chapter 2 is about measures. The measure already defined in chapter 1. In chapter 2, every linear &lt;strong&gt;functionals&lt;/strong&gt;, not combination, of a continuous function space on compact set (&lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;) (&lt;span class=&#34;math inline&#34;&gt;\(\Lambda f\)&lt;/span&gt;) represents the integration of the function (&lt;span class=&#34;math inline&#34;&gt;\(\int f du\)&lt;/span&gt;) (Riesz representation theorem). Let X be a locally compact Hausdorf space, and let &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; be a positive linear functional on &lt;span class=&#34;math inline&#34;&gt;\(C_c(X)\)&lt;/span&gt;. Then there exist a &lt;span class=&#34;math inline&#34;&gt;\(\sigma-algebra\)&lt;/span&gt; in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; which contains all Borel sets in &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, and there exists a unique positive measure &lt;span class=&#34;math inline&#34;&gt;\(mu\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}\)&lt;/span&gt; which represents &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; in the sense that (a) &lt;span class=&#34;math inline&#34;&gt;\(\Lambda f = \int f d \mu\)&lt;/span&gt; for every &lt;span class=&#34;math inline&#34;&gt;\(f \in C_c(X)\)&lt;/span&gt; and following additional properties:&lt;br /&gt;
(b) &lt;span class=&#34;math inline&#34;&gt;\(\mu(K) &amp;lt; \infty\)&lt;/span&gt; for every compact set &lt;span class=&#34;math inline&#34;&gt;\(K \subset X\)&lt;/span&gt;.&lt;br /&gt;
(c) For every &lt;span class=&#34;math inline&#34;&gt;\(E \in \mathfrak{M}\)&lt;/span&gt;, we have &lt;span class=&#34;math display&#34;&gt;\[ \mu(E) = inf\{\mu(V): E in V, V open\} \]&lt;/span&gt;.&lt;br /&gt;
(d) The relation &lt;span class=&#34;math display&#34;&gt;\[\mu(E)=sup\{\mu(K): K \in E, K compact\}\]&lt;/span&gt;&lt;br /&gt;
holds for every open set &lt;span class=&#34;math inline&#34;&gt;\(E\)&lt;/span&gt;, and for every &lt;span class=&#34;math inline&#34;&gt;\(E \in M\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(\mu(E) &amp;lt; \infty\)&lt;/span&gt;.&lt;br /&gt;
(e) If &lt;span class=&#34;math inline&#34;&gt;\(E \in \mathfrak{M}, A subset E\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mu(E) = 0\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(A \in \mathfrak{M}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Riesz theorem is about linear functional &lt;span class=&#34;math inline&#34;&gt;\(\Lambda\)&lt;/span&gt; is equivalently replaced with choosing measure &lt;span class=&#34;math inline&#34;&gt;\(\mu(E)=sup\{\Lambda f: f \prec V\}\)&lt;/span&gt;. Note &lt;span class=&#34;math inline&#34;&gt;\(sup \{\int^1_0 f(x)dx = \Lambda f: f \prec V, V (0,1) \} = 1\)&lt;/span&gt;. The notion of &lt;span class=&#34;math inline&#34;&gt;\(\prec\)&lt;/span&gt; include &lt;span class=&#34;math inline&#34;&gt;\(0 \le f \le 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I confused &lt;span class=&#34;math inline&#34;&gt;\(C_c(X)\)&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Abstract integration</title>
      <link>/post/abstract-integration/</link>
      <pubDate>Wed, 09 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/post/abstract-integration/</guid>
      <description>


&lt;p&gt;This is a note for Rudin’s real and complex analysis chapter 1. The key concepts are &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;-algebra, measure (&lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;) zero, and linear combination. The three concepts bring me abstract integration. The &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;-algebra makes that countable sum and measure of complement (subtract measure) can be possible. Measure zero completes the system. linear combination integrates a measurable function.&lt;/p&gt;
&lt;p&gt;After a measure space established, Lebesgue’s monotone convergence theorem, Fatou’s lemma, and Lebesgue’s dominant convergence theorem follow. Although three theorems do not contain &lt;strong&gt;integral&lt;/strong&gt; in their name, they insist that pointwise convergent sequence of functions is also converging their integral of the limit of functions. Lebesgue’s monotone convergence theorem requires a monotonous increment of series of functions and Lebesgue’s dominant convergence theorem requires upper bound function. Fatou’s lemma is the inequality of the lower limit.&lt;/p&gt;
&lt;p&gt;Lebesgue’s monotone convergence theorem can be proved by the fact all &lt;span class=&#34;math inline&#34;&gt;\(L_1(\mu)\)&lt;/span&gt; has convergent simple function sequence. &lt;span class=&#34;math inline&#34;&gt;\(f_n\)&lt;/span&gt; &amp;gt; simple functions holds inequality of &lt;span class=&#34;math inline&#34;&gt;\(\lim\int f_n \geq \lim\int S_n = \int f\)&lt;/span&gt;. Fatous’s lemma &lt;span class=&#34;math inline&#34;&gt;\(\lim \inf \int \lvert fn - f \rvert \leq\int \lim \inf \lvert f_n - f \rvert\)&lt;/span&gt; is used for proof of Lebesgue’s dominant convergence.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Compactness</title>
      <link>/post/math/compactness/</link>
      <pubDate>Sat, 07 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/post/math/compactness/</guid>
      <description>


&lt;p&gt;The compact is a property of space. In a nutshell, if space is compact, we can treat the space be a &lt;strong&gt;finite&lt;/strong&gt; because space has a &lt;strong&gt;finite subcover&lt;/strong&gt;. A continuous function on a compact space is uniformly continuous.&lt;/p&gt;
&lt;p&gt;Heine-Borel theorem describes the condition of compactness of &lt;strong&gt;finite&lt;/strong&gt; dimensional space. &lt;strong&gt;Closed and bounded&lt;/strong&gt; But the Heine-Borel theorem does not hold in an &lt;strong&gt;infinite-dimensional&lt;/strong&gt; space. We need another condition.&lt;/p&gt;
&lt;p&gt;Previously, the compact space can be finite by taking subcover. The infinite-dimensional space can be finite by projection to finite dimension. If we could make as small as possible (i.e. &lt;span class=&#34;math inline&#34;&gt;\(\epsilon &amp;gt; 0\)&lt;/span&gt; ) the norm of &lt;span class=&#34;math inline&#34;&gt;\((X\backslash(1-P)\)&lt;/span&gt;, the compactness is achieved.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functional analysis</title>
      <link>/post/math/functional-analysis/</link>
      <pubDate>Fri, 30 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/math/functional-analysis/</guid>
      <description>


&lt;p&gt;Differential equation solution is infinite function series. The infinite function series can be a sort of linear combination of countable function vector, in terms of linear algebra. This raises the problem of the analysis of function. The problem includes a distance of two functions, ie norm, completeness (Banach space). Because the series adds the last term without changing the existing terms, orthogonality is required to make a linear combination of countable functional vector becomes infinite function series (Hilbert space).&lt;/p&gt;
&lt;p&gt;To make a space of continous function on a compact interval &lt;span class=&#34;math inline&#34;&gt;\(C(I)\)&lt;/span&gt; be complete (Banach space), take the maximum norm. To make a space of &lt;span class=&#34;math inline&#34;&gt;\(C(I)\)&lt;/span&gt; orthogonal, take the norm associated with inner product &lt;span class=&#34;math inline&#34;&gt;\(\| f \| := \sqrt{ \langle f,f \rangle}\)&lt;/span&gt;. The inner product is a map of symmetric sesquilinear form (Hermitian form). In &lt;span class=&#34;math inline&#34;&gt;\(C(I)\)&lt;/span&gt;, the inner product is &lt;span class=&#34;math display&#34;&gt;\[ \langle f,g \rangle := \int_{a}^{b} f^{*} (x)g(x)dx \]&lt;/span&gt;
But the &lt;span class=&#34;math inline&#34;&gt;\(C(I)\)&lt;/span&gt; with the norm associated with an inner product is not complete. The complete can be achieved by extension to the Lebesgue integrable function. The right norm and the right functional set have special properties like complete and orthogonality.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Limit</title>
      <link>/post/math/limit/</link>
      <pubDate>Tue, 20 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/math/limit/</guid>
      <description>


&lt;p&gt;A sequence can be defined as a function on the domain of natural number like &lt;span class=&#34;math inline&#34;&gt;\(1, 1/2, 1/3 ... 1/n\)&lt;/span&gt;. This sequence approach to the 0, but never touch the 0. However, people can not take their desire to link the sequence and the 0. Because &lt;span class=&#34;math inline&#34;&gt;\(\infty\)&lt;/span&gt; is not a member of the natural number even real number, another concept is necessary to link the sequence and the 0. It is the limit. &lt;span class=&#34;math display&#34;&gt;\[ \lim{n\to\infty} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above sequence approach to the 0. But does all sequences approach to some points? What if the sequence is &lt;span class=&#34;math inline&#34;&gt;\(1/n\)&lt;/span&gt; if &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is not multiple of 100, 0.001 if n is multiple of 100.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/math/2019-08-22-limit_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Its approach to zero except at every multiple of 10. The &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; is used for the definition of limit to exclude this example.&lt;/p&gt;
&lt;p&gt;The sequence &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is converges the limit &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt; if for every positive &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt;, natural number &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is present such that &lt;span class=&#34;math inline&#34;&gt;\(\vert a-Sn \vert &amp;lt;\epsilon\)&lt;/span&gt; is true in every &lt;span class=&#34;math inline&#34;&gt;\(n&amp;gt;N\)&lt;/span&gt;. Otherwise, the limit is not defined and the sequence is divergent.&lt;/p&gt;
&lt;p&gt;In topological space, the &lt;span class=&#34;math inline&#34;&gt;\(\epsilon\)&lt;/span&gt; becomes the neighborhood.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NGS interpretation database and search</title>
      <link>/post/r/ngs-interpretation-database-and-search/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/r/ngs-interpretation-database-and-search/</guid>
      <description>


&lt;p&gt;An NGS pathology report contains an interpretation section to describe the clinical interpretation of the found genomic variants of the patient’s cancer sample. The interpretation is different from each variant and each cancer type or the other clinical factors. The pathologists describe the interpretation and archive those in certain methods including tables like excel.&lt;/p&gt;
&lt;p&gt;I make a shinyapp to archive the interpretations to a database and search them.&lt;br /&gt;
&lt;a href=&#34;https://jkang.shinyapps.io/Interpretation/&#34; class=&#34;uri&#34;&gt;https://jkang.shinyapps.io/Interpretation/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Carathéodory&#39;s extension theorem</title>
      <link>/post/math/caratheodory-s-extension-theorem/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/math/caratheodory-s-extension-theorem/</guid>
      <description>


&lt;p&gt;The studying sometimes starts with learning of boring preceding concepts. The highlight comes later. In history, the highlight concepts or the important problem were centered and the supporting concepts or lemmas followed. One of the central ideas of analysis is &lt;strong&gt;extension&lt;/strong&gt;. The set of a rational number (&lt;span class=&#34;math inline&#34;&gt;\(\mathbb{Q}\)&lt;/span&gt;) extends to the real line &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{R}\)&lt;/span&gt;. The Jordan measurable sets extend to the Lebesgue measurable sets ( &lt;span class=&#34;math inline&#34;&gt;\(\sigma -algebra\)&lt;/span&gt; ).&lt;/p&gt;
&lt;p&gt;The outer measure can measure &lt;strong&gt;all&lt;/strong&gt; subsets of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, whereas measure can only measure a &lt;span class=&#34;math inline&#34;&gt;\(\sigma -algebra\)&lt;/span&gt; of measure set. The Carathéodory measurability defines the condition to make a &lt;span class=&#34;math inline&#34;&gt;\(\sigma -algebra\)&lt;/span&gt;.&lt;br /&gt;
&lt;span class=&#34;math display&#34;&gt;\[ \mu^*(A) = \mu^*(A \cap E) + \mu^*(A \cap E^c) \]&lt;/span&gt;
The Carathéodory extension theorem defines a condition to make an outer measure to a measure. The condition is that the outer measure applies to the Carathéodory measurable set (&lt;span class=&#34;math inline&#34;&gt;\(\sigma - algebra\)&lt;/span&gt;). (Torrence Tao, An introduction to measure theory)&lt;/p&gt;
&lt;p&gt;In the Riesz representation therorem, &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}_F\)&lt;/span&gt; extends to &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}\)&lt;/span&gt;. The outer measure is &lt;span class=&#34;math inline&#34;&gt;\(\mu(E) = sup\ \ \{\mu(K): K \subset E,\ \ K \ \ compact \}\)&lt;/span&gt; for every &lt;span class=&#34;math inline&#34;&gt;\(E \subset X\)&lt;/span&gt; analogus to Jordan outer measure. &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}_F\)&lt;/span&gt; is collection of subset &lt;span class=&#34;math inline&#34;&gt;\(E \subset X\)&lt;/span&gt; satisfying &lt;span class=&#34;math inline&#34;&gt;\(\mu (E) &amp;lt; \infty\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu (V) = sup \{\Lambda f:f\prec V \}\)&lt;/span&gt; analogus to Jordan inner measure. Thus &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}_F\)&lt;/span&gt; is analous to Jordan measurable set. &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}\)&lt;/span&gt; is collection of subset &lt;span class=&#34;math inline&#34;&gt;\(E \subset X\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(E \cap K \ \in \ \ \mathfrak{M}_F\)&lt;/span&gt; for every compact &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;. This is the Carathéodory measurability. So the &lt;span class=&#34;math inline&#34;&gt;\(\mu(E)\)&lt;/span&gt; on the &lt;span class=&#34;math inline&#34;&gt;\(\mathfrak{M}\)&lt;/span&gt; becomes measure. (Rudin’s Real and complex analysis)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis</title>
      <link>/post/math/analysis/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/post/math/analysis/</guid>
      <description>


&lt;p&gt;The reproducing kernel hilbert space (RKHS) was my motivation to study analysis. The hilbert space is a orthogonal normed vector space. I still do not know about the meaning of “reproducing kernal”. The RKHS appeared in the book titled &lt;em&gt;An Introduction to Statisitical Learning&lt;/em&gt; written by Hastie.&lt;/p&gt;
&lt;p&gt;I began to google the meaning of the spaces such as the Hilbert, Banarch. I decided to read the &lt;em&gt;Understaing Analysis&lt;/em&gt; written by Abbott. The &lt;em&gt;Understaing Analysis&lt;/em&gt; was give me many intuitions of analysis and encouraged me to study further. The next book was Rudin’s &lt;em&gt;Functional Analysis&lt;/em&gt;. I realized I need to go upstream to complex Analysis, topology and measure.&lt;/p&gt;
&lt;p&gt;During the journey of exploring the analysis, I skipped proving of theorems or solving exercises. But space between lines is coming. I’m realizing that I prove the spaces. &lt;span class=&#34;math display&#34;&gt;\[\Sigma^{k}_{j=1} {\lvert}a_j-a^{n}_j{\rvert}\le\epsilon \]&lt;/span&gt; This holds for all finite &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, we even have &lt;span class=&#34;math inline&#34;&gt;\({\lVert} a-a_n {\rVert} _{1}\le\epsilon\)&lt;/span&gt;. This is on the way of the proof of &lt;span class=&#34;math inline&#34;&gt;\(l^{1}(\mathbb{N})\)&lt;/span&gt; of all complex-valued sequences &lt;span class=&#34;math inline&#34;&gt;\(a=(a_j)^{\infty}_{j=1}\)&lt;/span&gt; for which the norm &lt;span class=&#34;math inline&#34;&gt;\({\lVert} a {\rVert} _{1}\ := \Sigma^{\infty}_{j=1} {\lvert}a_j{\rvert}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I could not just accept that the finite sum of each small differences &lt;span class=&#34;math inline&#34;&gt;\(\le \epsilon\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\({\lvert}a_j - a^n_j{\rvert}\)&lt;/span&gt; holds to the infinite sum. The infinite sum is a infinte series. If the infinite series is less than or equel to zero, then it converses to the zero. If the finite sum is &lt;span class=&#34;math inline&#34;&gt;\(\le \epsilon\)&lt;/span&gt; holds every &lt;span class=&#34;math inline&#34;&gt;\(\mathbb{N}\)&lt;/span&gt;, by definition the infinite sum is also &lt;span class=&#34;math inline&#34;&gt;\(\le \epsilon\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It takes long time to grasp the subtle mathmatical systems. For example, a series is a number in a scalar field, a sequence is a ordered set. However the long time makes the math become familar and finally will firmly grasp the subtle concepts.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

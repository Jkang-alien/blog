<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Information theory | Jun&#39;s Blog</title>
    <link>/tags/information-theory/</link>
      <atom:link href="/tags/information-theory/index.xml" rel="self" type="application/rss+xml" />
    <description>Information theory</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 31 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Information theory</title>
      <link>/tags/information-theory/</link>
    </image>
    
    <item>
      <title>Information</title>
      <link>/post/information/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/information/</guid>
      <description>


&lt;p&gt;Information relates to uncertainty. The Shannon information content of an outcome &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(h(x)=-log_{2}P(x)\)&lt;/span&gt;. The rare event has larger information than a common event. The unit of information is a bit (binary digit). Coding is a mapping from an outcome of an ensemble to binary digits &lt;span class=&#34;math inline&#34;&gt;\(\{0,1\}^+\)&lt;/span&gt;. A symbol code is a code for a &lt;strong&gt;single&lt;/strong&gt; ensemble. A block code is a code for a &lt;strong&gt;sequence&lt;/strong&gt; ensemble. A set of sequences of the ensemble has a typical subset. The cardinality of a typical set is &lt;span class=&#34;math inline&#34;&gt;\(2^{H_{2}X}\)&lt;/span&gt;. We can reduce a code length by mapping codes to only a typical set (the source coding theorem). The prefix code is an optimal symbol code. The Kraft inequality is the condition of prefix code &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{i}2^{-l_{i}} \le 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The noisy-channel coding theorem describes the possible rate and block code length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. If the block code length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is long enough, the channel looks like the noisy typewriter and arbitrary block error rate can be achieved with rate. The maximum rate is the capacity &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; of the channel. If the rate is small enough, the typical set of the output of the channel can be mapped for the typical set of input without overlap.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Entropy</title>
      <link>/post/entropy/</link>
      <pubDate>Sun, 15 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/entropy/</guid>
      <description>


&lt;p&gt;This is a note for Elements of information theory of Thomas M. Cover.&lt;/p&gt;
&lt;p&gt;The entropy (&lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;) is a measure of uncertainty of a variable which is the answer to what is the ultimate data compression. Is the conditional probability &lt;span class=&#34;math inline&#34;&gt;\(p(x|y)\)&lt;/span&gt; considered as a probability of the “conditional variable” &lt;span class=&#34;math inline&#34;&gt;\((X|Y=y)\)&lt;/span&gt;? Yes, it is the subset of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y=y\)&lt;/span&gt;. If you sum all of the subset probabilities, it becomes the cardinality of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. Thus if you make that become 1, the conditional probability should be multiplied with &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt;. The conditional probability is larger than joint probabilities &lt;span class=&#34;math inline&#34;&gt;\(p(x,y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let’s think about the entropy of a joint random variable &lt;span class=&#34;math inline&#34;&gt;\((X,Y)\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; are correlated, the entries of the contingent table of &lt;span class=&#34;math inline&#34;&gt;\(p(x,y)\)&lt;/span&gt; are concentrated at some points that mean lager or smaller probabilities than a product of &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p(y)\)&lt;/span&gt;. The joint probability is a product of probability of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(p(x)\)&lt;/span&gt; and conditional probability &lt;span class=&#34;math inline&#34;&gt;\(p(X|Y)\)&lt;/span&gt;. The conditional entropy is the expectation of a random conditional variable (conditional entropy). The conditional entropy does not mean the entropy of a subset of &lt;span class=&#34;math inline&#34;&gt;\(X|Y=y\)&lt;/span&gt;. It is a measure of uncertainty of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; has the &lt;strong&gt;information&lt;/strong&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, the entropy of &lt;span class=&#34;math inline&#34;&gt;\(X|Y\)&lt;/span&gt; is less than &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The conditional entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X|Y)\)&lt;/span&gt; is subtract the entropy of Y &lt;span class=&#34;math inline&#34;&gt;\(H(Y)\)&lt;/span&gt; from joint entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X,Y)\)&lt;/span&gt;. The joint probability &lt;span class=&#34;math inline&#34;&gt;\(p(x,y)\)&lt;/span&gt; is the product of probability of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; and conditional probability &lt;span class=&#34;math inline&#34;&gt;\(p(x|Y=y)\)&lt;/span&gt;. This is chain rule of joint entropy &lt;span class=&#34;math inline&#34;&gt;\(H(X,Y) = H(Y) + H(X|Y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The chain rule is converting a joint variable to the sum of conditional random variables. The joint variable is the sum of conditional random variables. This is can be applied in the entropy, the information, and the relative entropy.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

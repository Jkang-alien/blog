<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine learning | Jun&#39;s Blog</title>
    <link>/tags/machine-learning/</link>
      <atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 14 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Machine learning</title>
      <link>/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>Laplace transformation</title>
      <link>/post/laplace-transformation/</link>
      <pubDate>Sat, 14 Nov 2020 00:00:00 +0000</pubDate>
      <guid>/post/laplace-transformation/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The Fourier series represents a periodic function as a descrete vectors. The Fourier transformation turns a time domain non-periodic function into a frequency domain continuous function. The Fourier series and transformation change a single time base &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; into infinite frequency basis &lt;span class=&#34;math inline&#34;&gt;\(e^{inx}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(e^{iwx}\)&lt;/span&gt;. The function on infinite basis domain can be represented by a vector or a function of basis domain &lt;span class=&#34;math inline&#34;&gt;\(v_{n}\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(f(w)\)&lt;/span&gt;. This is a coefficients of Fourier series or Fourier transformation.&lt;/p&gt;
&lt;p&gt;The basis of Fourier transformation is pure frequency &lt;span class=&#34;math inline&#34;&gt;\(e^{iw}\)&lt;/span&gt;. The domain of Laplace transfomation is frequency &lt;span class=&#34;math inline&#34;&gt;\(w\)&lt;/span&gt; and damping component &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; which compose damping ocilation function, &lt;span class=&#34;math inline&#34;&gt;\(e^{s} = e^{(iw+\sigma)}\)&lt;/span&gt;. The function which represent Laplace transformation &lt;span class=&#34;math inline&#34;&gt;\(F(s)\)&lt;/span&gt; is a function of complex domain &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;. The Fourier transformation is a special Laplace transformation of no damping term &lt;span class=&#34;math inline&#34;&gt;\(s = 0 \cdot \sigma +iw\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;periodic&lt;/strong&gt; function can be represented by a series not a continuous function. A condition makes a function can be represented by pure frequency domain i.e. Fourier transformation, not a complex domain i.e. Laplace transformation. The condition is&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;from wikipedia &lt;a href=&#34;https://en.wikipedia.org/wiki/Laplace_transform#Fourier_transform&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Laplace_transform#Fourier_transform&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;math&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
  \widehat{f}(\omega) &amp;amp;= \mathcal{F}\{f(t)\} \\[4pt]
                  &amp;amp;= \mathcal{L}\{f(t)\}|_{s = i\omega}  =  F(s)|_{s = i \omega} \\[4pt]
                  &amp;amp;= \int_{-\infty}^\infty e^{-i \omega t} f(t)\,dt~.
\end{align}\]&lt;/span&gt;&lt;/math&gt;&lt;/p&gt;
&lt;p&gt;Laplace transformation makes a differential equation to an algebra equation.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Laplace transformation\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L}[f(t)] = F(s) = \int_{t=0}^{\infty} f(t)e^{-st}dt
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[Transfer function\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
H(s) = Y(s)/X(s)
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
Y(s) = H(s)X(s)  
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X(s)\)&lt;/span&gt; are Laplace transformed &lt;span class=&#34;math inline&#34;&gt;\(y(t)\)&lt;/span&gt;, i.e. solution and &lt;span class=&#34;math inline&#34;&gt;\(f(t)\)&lt;/span&gt; i.e. input.&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; which represents coefficients of damped frquency basis &lt;span class=&#34;math inline&#34;&gt;\(e^{\sigma + iw}\)&lt;/span&gt;. We are not looking for the solution &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt;. We are looking for the inverse Laplace transformation of &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt;. The inverse Laplace transformation turns a function &lt;span class=&#34;math inline&#34;&gt;\(Y(s)\)&lt;/span&gt; with infinite damped frquency basis &lt;span class=&#34;math inline&#34;&gt;\(e^{\sigma + iw}\)&lt;/span&gt; to the solution of linear differential equation &lt;span class=&#34;math inline&#34;&gt;\(y(t)\)&lt;/span&gt; that is a function with a single domain basis &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Laplace transformation has poles that blow up at a point. The poles were determined by constants of differential equation and the input term.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convolution and Fourier transformation</title>
      <link>/post/convolution-and-fourier-transformation/</link>
      <pubDate>Thu, 22 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/post/convolution-and-fourier-transformation/</guid>
      <description>


&lt;p&gt;Convolution is a vector operation on two vectors.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Convolution \\ c * d = d*c \\ (c*d)_n = \Sigma_{i+j} c_i d_j = \Sigma_i c_i d_{n-i}.\]&lt;/span&gt;
This is multiplying polynomials. The parameters of multiplied polynomial become convolution of two polynomials. Fourier transformation expands x base to infinite exponential basis &lt;span class=&#34;math inline&#34;&gt;\(e^{iwk}\)&lt;/span&gt;. The &lt;strong&gt;multiplication on x (time) space&lt;/strong&gt; becomes &lt;strong&gt;convolutionn on k (frequency) space&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;If time space is periodic, its Fourier transformation is discrete i.e. Fourier series. If time space is non-periodic, its Fourier transformation is continuous Fourier transformation.&lt;/p&gt;
&lt;p&gt;The Fourier transformation is dual. The relations of &lt;strong&gt;multiplication and convolution&lt;/strong&gt; and &lt;strong&gt;periodic and discrete&lt;/strong&gt; are dual in time space and frequency space.&lt;/p&gt;
&lt;p&gt;Fourier transformation is changing basis. The changing basis can be done by inner product (for vector space) or integration (function space) with new basis in which are we want move to space. This is why Fourier transformation coefficients calculated by integration with function multiplying basis &lt;span class=&#34;math inline&#34;&gt;\(e^{iwk}\)&lt;/span&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lagrange dual problem and conjugate function</title>
      <link>/post/lagrange-dual-problem-and-conjugate-function/</link>
      <pubDate>Sun, 13 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/lagrange-dual-problem-and-conjugate-function/</guid>
      <description>


&lt;p&gt;The optimization problem have two components that are objective function &lt;span class=&#34;math inline&#34;&gt;\(f_0 : \mathbb R ^n \rightarrow \mathbb R\)&lt;/span&gt; and the constraints. The objective function and constraints keep in check each other and make balance at saddle point i.e. optimal point. The dual (Lagrange) problem of the optimal problem also solve the optimization problem by making low boundary.&lt;/p&gt;
&lt;p&gt;The dual problem can be explained as a conjugate function &lt;span class=&#34;math inline&#34;&gt;\(f^* = \sup (x^Ty-f(x))\)&lt;/span&gt;. The Lagrangian is &lt;span class=&#34;math inline&#34;&gt;\(L(x, \lambda, \nu) = f_0(x) + \lambda f_1, + \nu f_2\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(f_0\)&lt;/span&gt; is the objective function, &lt;span class=&#34;math inline&#34;&gt;\(f_1\)&lt;/span&gt; is inequality constraints and &lt;span class=&#34;math inline&#34;&gt;\(f_2\)&lt;/span&gt; is equality constraints. The Lagrangian function is &lt;span class=&#34;math inline&#34;&gt;\(g(\lambda,nu) = \inf_{x}L(x, \lambda, \nu) = \inf_{x}(f_0(x) + \lambda f_{1} + \nu f_{2})\)&lt;/span&gt;. The second and third term of the Lagrangian function is can be rewriten as an inner product form &lt;span class=&#34;math inline&#34;&gt;\(x^{T}h(\lambda) + x^{T}i(\nu)\)&lt;/span&gt; and constant term with &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. Then the inner product term &lt;span class=&#34;math inline&#34;&gt;\(x^{T}h(\lambda) + x^{T}i(\nu)\)&lt;/span&gt; and objective term becomes a conjugate function.&lt;/p&gt;
&lt;p&gt;The conjugate function &lt;span class=&#34;math inline&#34;&gt;\(f^*(x)\)&lt;/span&gt; is similar in terms of balance and saddle point.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Approximation</title>
      <link>/post/approximation/</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/post/approximation/</guid>
      <description>


&lt;p&gt;The purpose of approximation is finding optimal point &lt;span class=&#34;math inline&#34;&gt;\(x^*\)&lt;/span&gt; i.e. &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x^*) = 0\)&lt;/span&gt;. We need a step/search direction &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt; and step size &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Taylor approximation has polynomial arguments that is a step and parameters of derivatives at the start point. The first degree of Taylor approximation has one adding term from start point &lt;span class=&#34;math inline&#34;&gt;\((x_0, F(x_0))\)&lt;/span&gt;. The adding term &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x) \Delta x\)&lt;/span&gt; is consistent with a parameter (gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x)\)&lt;/span&gt;) and a argument (step &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt;). The Taylor approximation does approximate &lt;span class=&#34;math inline&#34;&gt;\(F(x + \Delta x)\)&lt;/span&gt; for any search direction &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt;. We want to choose &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt; for the direction to the optimal point.&lt;/p&gt;
&lt;p&gt;The adding term of Taylor approximation &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x) \Delta x\)&lt;/span&gt; have level curve (level line). The smallest Euclidean norm of the level curve is achieved at the tangent. The gradient descent set the step to the gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x)\)&lt;/span&gt;. This makes the adding term biggest with Euclidean norm &lt;span class=&#34;math inline&#34;&gt;\(\Vert \nabla F(x) \Vert^2\)&lt;/span&gt; i.e. dual norm &lt;span class=&#34;math inline&#34;&gt;\(\Vert \nabla F(x) \Vert_*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Newton’s method is second degree of Taylor approximation &lt;span class=&#34;math inline&#34;&gt;\(F(x_0+\Delta x) \approx F(x_0) + \nabla F(x) \Delta x + 1/2\Delta x^T H \Delta x\)&lt;/span&gt;. We want to find &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt; to minimize the second degree of Taylor approximation. In this case, the minimizing step is tangent of first adding term &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x) \Delta x\)&lt;/span&gt; and second adding term &lt;span class=&#34;math inline&#34;&gt;\(\Delta x^T H \Delta x\)&lt;/span&gt; i.e. Steepest descent in H norm &lt;span class=&#34;math inline&#34;&gt;\(\Vert \cdot \Vert _H\)&lt;/span&gt;. The newton’s method can be thought as approximation of gradient &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(\nabla F(x_0 + \Delta x) \approx \nabla F(x_0) + H \Delta x = 0,\ \Delta x = -H^{-1} \nabla F(x_0)\)&lt;/span&gt;. This is also the derivative of second degree of Taylor approximation with respect to &lt;span class=&#34;math inline&#34;&gt;\(\Delta x\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;But the Taylor approximation is local. In addition to a step, a step size is needed. A step size determines how far the step taken. Backtracking line search has two constant parameters 0 &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; &amp;lt; 0.5, 0 &amp;lt; &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; &amp;lt; 1. The approximation is below the convex function. &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; tilts the slope i.e. gradient upside and the tilted approximation meets the convex function. &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; is the update rate of the step size until the the amount of the step is less than the point that tilted approximation meeets the convex function.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Differential equations and Fourier transformation</title>
      <link>/post/differential-equations-and-fourier-transformation/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/differential-equations-and-fourier-transformation/</guid>
      <description>


&lt;p&gt;Differential equations describe the change of state. The change relates to the state. The solutions of the differential equations are the status equations. The initial conditions set the time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and status &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. The boundary conditions are the value of boundary &lt;span class=&#34;math inline&#34;&gt;\(y_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(dy \over dt\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(= ay + q(t)\)&lt;/span&gt; starting from &lt;span class=&#34;math inline&#34;&gt;\(y(0)\)&lt;/span&gt; at $t=0. inital conditions &lt;span class=&#34;math inline&#34;&gt;\(t = 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y=1\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(q(t)\)&lt;/span&gt; is a input and &lt;span class=&#34;math inline&#34;&gt;\(y(t)\)&lt;/span&gt; is a response. If &lt;span class=&#34;math inline&#34;&gt;\(q(t)\)&lt;/span&gt; is delta function, the response is said &lt;strong&gt;Impulse response&lt;/strong&gt; &lt;span class=&#34;math display&#34;&gt;\[y&amp;#39; -ay = \delta (t) \\ y(t)=e^{at}\]&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The solutions are combination of particular solution and null solution &lt;span class=&#34;math inline&#34;&gt;\(y = y_t + y_n\)&lt;/span&gt;. The solution includes &lt;span class=&#34;math inline&#34;&gt;\(e^{at}\)&lt;/span&gt;. The differential equations can not be solved like polynomial equations, because the arguments of the differentia equation relate to each other by calculus in the background of the equation. They can not be treated as just different arguments. The &lt;strong&gt;Fourier transformation&lt;/strong&gt; puts the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and its derivative &lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;\)&lt;/span&gt; in the same functional space (Hilbert space). This transformation makes the differential equation problem to simple arithmetic problem.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fourier transformation &lt;span class=&#34;math inline&#34;&gt;\(F(x) = \Sigma ^{\infty}_{n=-\infty} c_{n}e^{inx}\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The basis of the Fourier transformation is &lt;span class=&#34;math inline&#34;&gt;\(e^{inx}\)&lt;/span&gt;. If the coefficients of the basis &lt;span class=&#34;math inline&#34;&gt;\(c_{n}\)&lt;/span&gt; decay fater, &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; becomes smooth. If the coefficients are constant, &lt;span class=&#34;math inline&#34;&gt;\(F(x)\)&lt;/span&gt; is delta function &lt;span class=&#34;math inline&#34;&gt;\(\delta(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The derivative &lt;span class=&#34;math inline&#34;&gt;\(dy \over dt\)&lt;/span&gt; is an linear transformation operator, i.e. inner product, because the &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y&amp;#39;\)&lt;/span&gt; are in functional space with same basis. The defivative can be represented as a matix &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;. The derivative matrix is antisymetric i.e. &lt;span class=&#34;math inline&#34;&gt;\(A^T = -A\)&lt;/span&gt; and the minus second derivative matrix &lt;span class=&#34;math inline&#34;&gt;\(-d^{2}/dx^{2}\)&lt;/span&gt; is symetic positive definite. &lt;span class=&#34;math inline&#34;&gt;\(AAf = -A^{T}Af\)&lt;/span&gt;. The meaning of transverse of a matrix is &lt;span class=&#34;math inline&#34;&gt;\((Ax)^{T}y = x^{T}(A^{T}y)\)&lt;/span&gt;. &lt;strong&gt;Dual and inner product&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/secondDifferenceMat.png&#34; alt=&#34;Second differnce matrix K&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Second differnce matrix K&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The second difference matrix solves discrete differential equations. The N eigenvectors of K are &lt;span class=&#34;math inline&#34;&gt;\(y_{n} = (sin\ n\pi \Delta x, sin\ 2n\pi \Delta x,\ ..., sin\ Nn\pi \Delta x)\)&lt;/span&gt;. The N eigen values of K are the positive numbers &lt;span class=&#34;math inline&#34;&gt;\(\lambda_{n} = 2-2cos {n \pi \over N+1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;How does exponent &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; mean in &lt;span class=&#34;math inline&#34;&gt;\(e^i\)&lt;/span&gt;? The exponent makes multiplication to addition. What does an imaginary exponent mean? The imaginary exponent tilts the value to a complex plane. If the base is natural base &lt;span class=&#34;math inline&#34;&gt;\(e\)&lt;/span&gt;, the value of &lt;span class=&#34;math inline&#34;&gt;\(e^i\)&lt;/span&gt; is in the unit circle of a complex plane. The cycle is &lt;span class=&#34;math inline&#34;&gt;\(2 \pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The Fourier transformation for solving the difference equation provoked the subject of functional analysis 200 years ago.&lt;/p&gt;
&lt;p&gt;Reference&lt;br /&gt;
Differential Equations and Linear Algebra, Gilbert Strang&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Information</title>
      <link>/post/information/</link>
      <pubDate>Sun, 31 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/information/</guid>
      <description>


&lt;p&gt;Information relates to uncertainty. The Shannon information content of an outcome &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(h(x)=-log_{2}P(x)\)&lt;/span&gt;. The rare event has larger information than a common event. The unit of information is a bit (binary digit). Coding is a mapping from an outcome of an ensemble to binary digits &lt;span class=&#34;math inline&#34;&gt;\(\{0,1\}^+\)&lt;/span&gt;. A symbol code is a code for a &lt;strong&gt;single&lt;/strong&gt; ensemble. A block code is a code for a &lt;strong&gt;sequence&lt;/strong&gt; ensemble. A set of sequences of the ensemble has a typical subset. The cardinality of a typical set is &lt;span class=&#34;math inline&#34;&gt;\(2^{H_{2}X}\)&lt;/span&gt;. We can reduce a code length by mapping codes to only a typical set (the source coding theorem). The prefix code is an optimal symbol code. The Kraft inequality is the condition of prefix code &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_{i}2^{-l_{i}} \le 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The noisy-channel coding theorem describes the possible rate and block code length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt;. If the block code length &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; is long enough, the channel looks like the noisy typewriter and arbitrary block error rate can be achieved with rate. The maximum rate is the capacity &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; of the channel. If the rate is small enough, the typical set of the output of the channel can be mapped for the typical set of input without overlap.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Taylor series</title>
      <link>/post/taylor-series/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/taylor-series/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x) = \sum_{k=0}^\infty c_k x^k = c_0 + c_1 x + c_2 x^2 + \dotsb. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is an approximation that is a function of h and derivatives of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; are elements of parameters.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(f(x \pm h) = f(x) \pm hf&amp;#39;(x) + \frac{h^2}{2}f&amp;#39;&amp;#39;(x) \pm \frac{h^3}{6}f&amp;#39;&amp;#39;&amp;#39;(x) + O(h^4)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s think about &lt;span class=&#34;math inline&#34;&gt;\(\sin(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ f(x) = \sin(x) \ f(0) = 0, f&amp;#39;(x)=\cos(x)\ f&amp;#39;(0)=1, f&amp;#39;&amp;#39;(x)=-\sin(x)\ f&amp;#39;&amp;#39;(0)=0 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align*} \sin(x) &amp;amp;= 0 + \frac{1}{1!}x + \frac{0}{2!}x^2 + \frac{-1}{3!}x^3 + \dotsb
&amp;amp;= x - \frac{x^3}{3!} + \frac{x^5}{5!} - \dotsb, \end{align*}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is approximation. Now &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; becomes &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; and parameters calculated from derivatives of &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; at &lt;span class=&#34;math inline&#34;&gt;\(0\)&lt;/span&gt;.&lt;br /&gt;
&lt;span class=&#34;math inline&#34;&gt;\(f(x \pm h) = f(x) \pm hf&amp;#39;(x) + \frac{h^2}{2}f&amp;#39;&amp;#39;(x) \pm \frac{h^3}{6}f&amp;#39;&amp;#39;&amp;#39;(x) + O(h^4)\)&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;/img/sine-better-models.png&#34; alt=&#34;https://betterexplained.com/articles/taylor-series/&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;&lt;a href=&#34;https://betterexplained.com/articles/taylor-series/&#34; class=&#34;uri&#34;&gt;https://betterexplained.com/articles/taylor-series/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Taylor series and Newton’s bionomial theorem explain the complex exponent.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\exp(z) = e^{z}, \ z = a+bi \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The imaginary exponent is hard to understand intuitively. The exponential function &lt;span class=&#34;math inline&#34;&gt;\(e^{x}\)&lt;/span&gt; on a complex domain can be regarded as a function exp(x) that behaves like exponential function, i.e. a product of functions is addion of arguments &lt;span class=&#34;math inline&#34;&gt;\(\exp(x) \exp(y) = \exp(x+y)\)&lt;/span&gt;. The product of &lt;span class=&#34;math inline&#34;&gt;\(\exp\)&lt;/span&gt; fucntion becomes addition of arguments by Newton’s binomical theorem. The costomary expression is &lt;span class=&#34;math inline&#34;&gt;\(e^{x}\)&lt;/span&gt;. This can be done when &lt;span class=&#34;math inline&#34;&gt;\(\exp(x) = \Sigma ^{\infty}_{n=0} \frac {Z^{n}}{n!}\)&lt;/span&gt; The taylor series with repidly decaying pactorial coefficients &lt;span class=&#34;math inline&#34;&gt;\(n!\)&lt;/span&gt;. This series converges absolutely for every complex &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; and converges uniformly on every bounded subset of the complex plain. Rudin’s Real and complex analysis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodel and glmnet</title>
      <link>/post/tidymodel-and-glmnet/</link>
      <pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/post/tidymodel-and-glmnet/</guid>
      <description>


&lt;p&gt;When the penalized generalize linear model (Lasso or Ridge) is processed in the tidymodel environment, finalizing the hyperparameter (lambda) and getting coefficients of the final model are confusing. Here is an example. This example predicts PIK3CA mutation status by gene expression data. TCGA breast cancer dataset is used.&lt;/p&gt;
&lt;div id=&#34;modeling&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Modeling&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(glmnet)
library(themis)

set.seed(930093)
cv_splits &amp;lt;- rsample::vfold_cv(trainset_ahDiff, strata = PIK3CA_T)
mod &amp;lt;- logistic_reg(penalty = tune(),
                    mixture = tune()) %&amp;gt;%
  set_engine(&amp;quot;glmnet&amp;quot;)

rec &amp;lt;- recipe(PIK3CA_T ~ ., data = trainset_ahDiff) %&amp;gt;%
  step_BoxCox(all_numeric()) %&amp;gt;%
  step_dummy(HISTOLOGICAL_DIAGNOSIS) %&amp;gt;%
  step_center(all_numeric()) %&amp;gt;%
  step_scale(all_numeric()) %&amp;gt;%
  step_smote(PIK3CA_T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wfl &amp;lt;- workflow() %&amp;gt;%
  add_recipe(rec) %&amp;gt;%
  add_model(mod)

glmn_set &amp;lt;- parameters(penalty(range = c(-5,1), trans = log10_trans()),
                       mixture())

glmn_grid &amp;lt;- 
  grid_regular(glmn_set, levels = c(7, 5))
ctrl &amp;lt;- control_grid(save_pred = TRUE, verbose = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Grid parameter search on 10-fold cross-validation with 5 repeats&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Dummy variable to control for histologic subtype&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;select-best-parameter&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Select best parameter&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glmn_tune &amp;lt;- 
  tune_grid(wfl,
            resamples = cv_splits,
            grid = glmn_grid,
            metrics = metric_set(roc_auc),
            control = ctrl)


best_glmn &amp;lt;- select_best(glmn_tune, metric = &amp;quot;roc_auc&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;finalizing&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Finalizing&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wfl_final &amp;lt;- 
  wfl %&amp;gt;%
  finalize_workflow(best_glmn) %&amp;gt;%
  fit(data = trainset_ahDiff)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;finalize_workflow()&lt;/code&gt; finalizes the model with selected optimal hyperparameters. However, the glmnet fits any lambda, not the indicated lambda. This was discussed at &lt;a href=&#34;https://github.com/tidymodels/parsnip/issues/195&#34; class=&#34;uri&#34;&gt;https://github.com/tidymodels/parsnip/issues/195&lt;/a&gt;. The glmnet is more efficient to fit all lambda than a single lambda. Thus tidymodel ignores the indicated lambda. This made the first confusion. &lt;strong&gt;The finalization can be finalized by predict in tidymodel environment.&lt;/strong&gt; Finalize with &lt;code&gt;predict&lt;/code&gt;. Note the last argument &lt;code&gt;penalty = 1&lt;/code&gt; of &lt;code&gt;stats::predict(wfl_final, type = &#34;prob&#34;, new_data = trainset_ahDiff, penalty = 1)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_predict &amp;lt;- stats::predict(wfl_final, type = &amp;quot;prob&amp;quot;, new_data = trainset_ahDiff, penalty = 1)
train_probs &amp;lt;- 
  predict(wfl_final, type = &amp;quot;prob&amp;quot;, new_data = trainset_ahDiff) %&amp;gt;%
  bind_cols(obs = trainset_ahDiff$PIK3CA_T) %&amp;gt;%
  bind_cols(predict(wfl_final, new_data = trainset_ahDiff))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Performance&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;conf_mat(train_probs, obs, .pred_class)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           Truth
## Prediction Wild Mutant
##     Wild    213     45
##     Mutant  123    158&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;autoplot(roc_curve(train_probs, obs, .pred_Mutant, event_level = &amp;quot;second&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-03-18-tidymodel-and-glmnet_files/figure-html/performance-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roc_auc(train_probs, obs, .pred_Mutant, event_level = &amp;quot;second&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 roc_auc binary         0.770&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because glmnet fits the whole path, there are whole coefficients in the glmnet fit object &lt;code&gt;wfl_final&lt;/code&gt;. This was the second confusion. How to get the final model coefficients is below.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;coefficients&#34; class=&#34;section level5&#34;&gt;
&lt;h5&gt;Coefficients&lt;/h5&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tidy(extract_model(wfl_final)) %&amp;gt;%
  filter(lambda &amp;gt; 0.98 &amp;amp; lambda &amp;lt; 1.01)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17 x 5
##    term                                           step estimate lambda dev.ratio
##    &amp;lt;chr&amp;gt;                                         &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 (Intercept)                                      55  -0.0630   1.00     0.123
##  2 C4A                                              55   0.0587   1.00     0.123
##  3 C5orf13                                          55   0.0587   1.00     0.123
##  4 CDSN                                             55   0.0706   1.00     0.123
##  5 CFB                                              55   0.0719   1.00     0.123
##  6 CYP21A2                                          55   0.0516   1.00     0.123
##  7 DGKE                                             55  -0.0709   1.00     0.123
##  8 FGD5                                             55   0.0670   1.00     0.123
##  9 GALNT10                                          55   0.0575   1.00     0.123
## 10 GOLM1                                            55   0.0689   1.00     0.123
## 11 GPX8                                             55   0.0657   1.00     0.123
## 12 KLK11                                            55   0.0145   1.00     0.123
## 13 NTN4                                             55   0.0578   1.00     0.123
## 14 SMYD3                                            55   0.0637   1.00     0.123
## 15 USP36                                            55  -0.0698   1.00     0.123
## 16 WBP2                                             55  -0.0652   1.00     0.123
## 17 HISTOLOGICAL_DIAGNOSIS_Infiltrating.Lobular.~    55  -0.0244   1.00     0.123&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Strong convexity and implications</title>
      <link>/post/strong-convexity-and-implications/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/strong-convexity-and-implications/</guid>
      <description>


&lt;p&gt;This is a summary of the Boyd convex optimization book. The strong convexity assumption can be useful to explain the iterative minimization algorithms like gradient descent, steepest descent, and Newton’s method.&lt;/p&gt;
&lt;p&gt;The smallest and largest eigen value of Hessian &lt;span class=&#34;math inline&#34;&gt;\(m \preceq \nabla^{2}f(x) \preceq M\)&lt;/span&gt; with norm of gradient &lt;span class=&#34;math inline&#34;&gt;\(\| \nabla f(x)\|_2\)&lt;/span&gt; determine the boundary of optimal value &lt;span class=&#34;math inline&#34;&gt;\(p^{*}\)&lt;/span&gt;. The condition number of &lt;strong&gt;cond&lt;/strong&gt;(&lt;span class=&#34;math inline&#34;&gt;\(C_\alpha\)&lt;/span&gt;) &lt;span class=&#34;math inline&#34;&gt;\(\leq {M \over m}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(C_\alpha\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;-sublevel. The condition number is ratio of largest eigen value to its smallest eigen value.&lt;/p&gt;
&lt;p&gt;When the Hessian is replaced with a constant &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;, the constants make the equality of Taylor’s theorem to inequality that makes lower and upper boundaries of &lt;span class=&#34;math inline&#34;&gt;\(p^*\)&lt;/span&gt;. The difference between the approximation and &lt;span class=&#34;math inline&#34;&gt;\(p^*\)&lt;/span&gt; is determined by $ f(x)$ and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\nabla f(x)\)&lt;/span&gt; is small and the gap is so. If Hessian (&lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt;) is large, the gap is small.&lt;/p&gt;
&lt;p&gt;Because the second degree of Tayler’s expansion is quadratic, at near the optimal point, the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;-sublevel is ellipsoid. The condition number &lt;strong&gt;cond&lt;/strong&gt;(&lt;span class=&#34;math inline&#34;&gt;\(C_{\alpha}\)&lt;/span&gt;), geometrically, represents anisometry or eccentricity&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodel</title>
      <link>/post/tidymodel/</link>
      <pubDate>Fri, 31 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/tidymodel/</guid>
      <description>


&lt;div id=&#34;machine-learning-and-tidymodel&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Machine Learning and Tidymodel&lt;/h3&gt;
&lt;div id=&#34;model-setting-parsnip&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Model setting, {Parsnip}&lt;/h4&gt;
&lt;p&gt;Rpackage Parsnip standardizes model specification. Tidymodel follows the concept of lazy evaluation of the tidyverse. Parsnip sets unified specifications and lately evaluates.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;feature-engineering-recipes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Feature engineering, {Recipes}&lt;/h4&gt;
&lt;p&gt;Recipes make preprocessing easy with &lt;code&gt;step_()&lt;/code&gt; functions. Recipes after specification calculate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;resampling-rsample&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Resampling, {rsample}&lt;/h4&gt;
&lt;p&gt;To choose a model and hyperparameters, we must validate the different models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;making-hyperparameter-set-dials&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Making hyperparameter set, {dials}&lt;/h4&gt;
&lt;p&gt;The Rpackage {dials} set hyperparameter similarily with {Parsnip}. {Dials} standadize parameter of each modeling algorithm.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;set-modeling-process-workflowr&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Set modeling process, {Workflowr}&lt;/h4&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-models-with-hyperparameter-tunes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Fit models with hyperparameter, {tunes}&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Applied Machine Learning Workshop RStudio Conference 2020</title>
      <link>/post/applied-machine-learning-workshop-rstudio-conference-2020/</link>
      <pubDate>Sun, 26 Jan 2020 00:00:00 +0000</pubDate>
      <guid>/post/applied-machine-learning-workshop-rstudio-conference-2020/</guid>
      <description>


&lt;p&gt;This is a note of applied machine learning workshop RStudion conference 2020&lt;/p&gt;
&lt;p&gt;Why is it hard to predict (domain knowledge).&lt;/p&gt;
&lt;p&gt;purrr::map allows inline code.&lt;/p&gt;
&lt;p&gt;purrr::map and tidyr::nest covered because they are used in resample or tune.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-26-applied-machine-learning-workshop-rstudio-conference-2020_files/figure-html/explore-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Skew data might be looking outlier.&lt;/p&gt;
&lt;p&gt;People look at data in many different ways like outliers, missingness, correlation, and suspicion of an important variable.&lt;/p&gt;
&lt;p&gt;The ggplot is good to explore variables adding geoms changing plot.&lt;/p&gt;
&lt;p&gt;Machine learning is creative because you can do many different ways such as which variable should be included.&lt;/p&gt;
&lt;p&gt;Model workflow: imputation -&amp;gt; transformation -&amp;gt; filter -&amp;gt; model&lt;/p&gt;
&lt;p&gt;Resampling avoids something like the garden of forking paths or p-hacking by honest feedback.&lt;/p&gt;
&lt;p&gt;The tuning parameter can be estimated &lt;strong&gt;analytically&lt;/strong&gt; or iteratively.&lt;/p&gt;
&lt;p&gt;Single validation set ??&lt;/p&gt;
&lt;p&gt;Resampling process questions&lt;/p&gt;
&lt;p&gt;Splitting data is confusing.&lt;/p&gt;
&lt;p&gt;The test set split preserves &lt;strong&gt;distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Interval and sampling are behind the scene of resampling.&lt;/p&gt;
&lt;p&gt;fomular x ~ . -something to remove&lt;/p&gt;
&lt;p&gt;Variable role: limited (can not be applied to hierarchical in Baysian??)&lt;/p&gt;
&lt;p&gt;Formula and XY interface is not fit in machine learning. The recipes package is a solution.&lt;/p&gt;
&lt;p&gt;The broom::glance (one-row summary of metrics, don’t trust this too much) tidy (coefficients) augment (by data points)&lt;/p&gt;
&lt;p&gt;The parsnip (unified interface) is a solution to a different interface.&lt;/p&gt;
&lt;p&gt;How to set specific settings using parsnip?&lt;/p&gt;
&lt;p&gt;Many other packages supporting parsnip. Look at the tidymodels packages.&lt;/p&gt;
&lt;p&gt;What is the difference between caret and parsnip?&lt;/p&gt;
&lt;p&gt;Caret~base R, parsnip~tidyverse&lt;/p&gt;
&lt;p&gt;The parsnip generalizes the interfaces and is easy to extend.&lt;/p&gt;
&lt;p&gt;A dummy variable is confusing in the interaction of terms and how to interpret coefficiency.&lt;/p&gt;
&lt;p&gt;The level with 0,0 allocation becomes a base variable in factor variable. A continuous variable can be categorized with recipe::step_discritize().&lt;/p&gt;
&lt;p&gt;A zero variance predictor includes no record or single value variable.&lt;/p&gt;
&lt;p&gt;Feature engineering problem
* Dummy variable&lt;br /&gt;
* Zero variance
* Standardize&lt;/p&gt;
&lt;p&gt;Customizing Step function&lt;/p&gt;
&lt;p&gt;The first level is a reference. There are ways to change the reference level. (Ordering factor level issues)&lt;/p&gt;
&lt;p&gt;People respond positively to the recipes.&lt;/p&gt;
&lt;p&gt;recipe -&amp;gt; prep -&amp;gt; juice/bake; define -&amp;gt; calculate -&amp;gt; processing (training or test set)&lt;/p&gt;
&lt;p&gt;The post-processed data set goes to fit() to establish a model&lt;/p&gt;
&lt;p&gt;Versioning model caching?&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_rec &amp;lt;- recipe(
  Sale_Price ~ Longitude + Latitude + Neighborhood, 
  data = ames_train
) %&amp;gt;%
  step_log(Sale_Price, base = 10) %&amp;gt;%
  
  # Lump factor levels that occur in 
  # &amp;lt;= 5% of data as &amp;quot;other&amp;quot;
  step_other(Neighborhood, threshold = 0.05) %&amp;gt;%
  
  # Create dummy variables for _any_ factor variables
  step_dummy(all_nominal()) %&amp;gt;%
  step_nzv(
    starts_with (&amp;quot;Neighborhood_&amp;quot;))

preped_data &amp;lt;- prep(mod_rec) 

preped_data %&amp;gt;%
  juice() %&amp;gt;%
  slice(1:5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 11
##   Longitude Latitude Sale_Price Neighborhood_Co… Neighborhood_Ol…
##       &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt;
## 1     -93.6     42.1       5.24                0                0
## 2     -93.6     42.1       5.39                0                0
## 3     -93.6     42.1       5.28                0                0
## 4     -93.6     42.1       5.29                0                0
## 5     -93.6     42.1       5.33                0                0
## # … with 6 more variables: Neighborhood_Edwards &amp;lt;dbl&amp;gt;,
## #   Neighborhood_Somerset &amp;lt;dbl&amp;gt;, Neighborhood_Northridge_Heights &amp;lt;dbl&amp;gt;,
## #   Neighborhood_Gilbert &amp;lt;dbl&amp;gt;, Neighborhood_Sawyer &amp;lt;dbl&amp;gt;,
## #   Neighborhood_other &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;step_dummy -&amp;gt; step_other/step_nzv()&lt;/p&gt;
&lt;p&gt;Step_dummy creates many dummy variables thus you should include the new dummy variables into the interaction terms or others or nzv. with starts_with() function.&lt;/p&gt;
&lt;p&gt;Resolving skewness: &lt;strong&gt;Box-cox&lt;/strong&gt;, &lt;strong&gt;inverse&lt;/strong&gt;, of course, log or square root&lt;/p&gt;
&lt;p&gt;parsnip model object includes original model results in &lt;code&gt;$fit&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The workflow replaces prep() -&amp;gt; juice() -&amp;gt; fit() with single call fit() and bake() -&amp;gt; predict() with predict ().&lt;/p&gt;
&lt;p&gt;The workflow needs &lt;code&gt;add&lt;/code&gt;ing model specification (parsnip) and preprocessing (recipes).&lt;/p&gt;
&lt;p&gt;Resampling is the best option to estimate the performance of a model.&lt;/p&gt;
&lt;p&gt;Resampling splits analysis set and assessment set on &lt;strong&gt;training set&lt;/strong&gt;. The final result is performance metrics.&lt;/p&gt;
&lt;p&gt;Selecting the resampling method relates to bias-variance trade-off. (Repeated CV)&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Model_selection#Criteria&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Model_selection#Criteria&lt;/a&gt; information criteria??&lt;/p&gt;
&lt;p&gt;Resampling does stratified sampling.&lt;/p&gt;
&lt;p&gt;The tuning metric tends to be smooth. The irregular metric result means high variance.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm&#34; class=&#34;uri&#34;&gt;http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Resampling spends memory. The vfold_cv() has a copy of the original data.&lt;/p&gt;
&lt;p&gt;Processing first or resampling first??&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Resample first&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If preprocessing is outside of resampling, You don’t know how the test set will be predicted.&lt;/p&gt;
&lt;p&gt;Is feature engineering stable or unstable?&lt;/p&gt;
&lt;p&gt;Can tune package select a good preprocessing process? Yes, it can.&lt;/p&gt;
&lt;p&gt;The upsampling issues and rare important data points&lt;/p&gt;
&lt;p&gt;The tuning hyperparameter (underfitting or overfitting).&lt;/p&gt;
&lt;p&gt;Hyperparameter types grid (regular, irregular), iterative&lt;/p&gt;
&lt;p&gt;Making function name: use underbar &lt;code&gt;_&lt;/code&gt; “Grid_regular”&lt;/p&gt;
&lt;p&gt;The pros and cons of regular and non-regular grid&lt;/p&gt;
&lt;p&gt;The non-regular grid efficient except some model can do trick.&lt;/p&gt;
&lt;p&gt;The parsnip standardizes and it changes default parameter ranges.&lt;/p&gt;
&lt;p&gt;The parameter tibble can not be subsetted. It will be issued in GitHub.&lt;/p&gt;
&lt;p&gt;People take care of the default value of parameters.&lt;/p&gt;
&lt;p&gt;Tidymodel delays evaluation. Set first and run later.&lt;/p&gt;
&lt;p&gt;The Splines fitting is wagged at the edge of the range. When trying to predict value out of range, warnings occur. Resampling can cause an error at the edge point of the assessment set.&lt;/p&gt;
&lt;p&gt;There is a note column in the result object.&lt;/p&gt;
&lt;p&gt;The sensitivity of parameters can be evaluated with ggplot. The scale should be adjusted when an outlier is present.&lt;/p&gt;
&lt;p&gt;Best fit can be choose&lt;/p&gt;
&lt;p&gt;Documentation&lt;/p&gt;
&lt;p&gt;Show best and select best (parameter)&lt;/p&gt;
&lt;p&gt;Tune tunes selected parameters. If you don’t tune parameters, the parameters go default value.&lt;/p&gt;
&lt;p&gt;The log ridership is wired because of a bimodal distribution. It will show skewed residual or something.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;!!stations&lt;/code&gt; indicates the environment of the station object, the station object is defined at recipe object, not the global environment.&lt;/p&gt;
&lt;p&gt;Regularization wins wrapping feature selection methods in terms of efficacy.&lt;/p&gt;
&lt;p&gt;If two predictors are highly correlated, the signs can change and the variance of coefficients inflates.&lt;/p&gt;
&lt;p&gt;L1 penalty does feature selection, the L2 penalty resolves correlation. Does mixture feature selection? Yes, the L1 component and lambda determine how many variables are kicked out, except pure ridge regression (ie. pure L2 penalty).&lt;/p&gt;
&lt;p&gt;Normalize or standardize on dummy variables?&lt;/p&gt;
&lt;p&gt;Lambda is a more important parameter than alpha in glm.&lt;/p&gt;
&lt;p&gt;Parallelism issue. GPU is good at linear algebra. Parallelism consumes memory because they copy data. Tuning is advantaged with parallelism because it uses for a loop. Unix is better than Windows in terms of parallelism.&lt;/p&gt;
&lt;p&gt;Inner join makes prediction tibble subsetting best.&lt;/p&gt;
&lt;p&gt;Repeated CV residual can be estimated with average predicting values.&lt;/p&gt;
&lt;p&gt;GCV is used to pruning the point.&lt;/p&gt;
&lt;p&gt;Is MARS greedy? Semi-greedy.&lt;/p&gt;
&lt;p&gt;MARS is struggling with colinearity. Use step_pca.&lt;/p&gt;
&lt;p&gt;Bayesian process, Gaussian process, Kernel function selection&lt;/p&gt;
&lt;p&gt;MARS hyperparameter space is high dimensional, thus the Gaussian process is better than the grid method in terms of efficiency. Grid methods for the Chicago data MARS model need more than 4000 points of the grid, although the grid search can use a parallel process.&lt;/p&gt;
&lt;p&gt;The Bayesian parameter searching process can be updated and pause.&lt;/p&gt;
&lt;p&gt;Classification Hard prediction vs soft prediction (probability)&lt;/p&gt;
&lt;p&gt;Accuracy can be estimated with hard prediction and ROC can be produced on soft prediction.&lt;/p&gt;
&lt;p&gt;Feature hashing is difficult to understand.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;recipe can be tune.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;C5.0 is boost_tree.&lt;/p&gt;
&lt;p&gt;Gitter discussion
&lt;a href=&#34;https://gitter.im/conf2020-applied-ml/community/archives/2020/01/28&#34; class=&#34;uri&#34;&gt;https://gitter.im/conf2020-applied-ml/community/archives/2020/01/28&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>/post/reproducing-kernel-hilbert-space/</link>
      <pubDate>Fri, 20 Dec 2019 00:00:00 +0000</pubDate>
      <guid>/post/reproducing-kernel-hilbert-space/</guid>
      <description>


&lt;p&gt;Finally arrive at reproducing kernel Hilbert space.
&lt;a href=&#34;https://nzer0.github.io/reproducing-kernel-hilbert-space.html&#34; class=&#34;uri&#34;&gt;https://nzer0.github.io/reproducing-kernel-hilbert-space.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The above post introduces RKHS in Korean. It was helpful. I had struggled to understand some concepts in RKHS. What does mean Hilbert space in terms of feature expansion? (&lt;span class=&#34;math inline&#34;&gt;\(f:\mathcal{X} \to \mathbb{R}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f \in \mathcal{H}_K\)&lt;/span&gt;) It was confusing the difference between &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; means the function in Hilbert space and &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt; is &lt;strong&gt;evaluation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I thought that the function can be represented by the inner product of the basis of feature space &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot,x)\)&lt;/span&gt; and coefficients &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt;, and the coefficients are vectors in feature space.&lt;/p&gt;
&lt;p&gt;The reproducing property of Kernel is &lt;span class=&#34;math inline&#34;&gt;\(\langle f, K(\cdot,x)\rangle_{\mathcal{H}} = f(x)\)&lt;/span&gt;. Thus &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot,x) \in \mathcal{H}_K\)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot,x)\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; specified function in Hilbert space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_K\)&lt;/span&gt; and an evaluator of the specific point x. This means the inner product of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(K_{x}\)&lt;/span&gt; is the value of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(f(x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In a nutshell, kenel method is a &lt;strong&gt;different way of evaluating f in a specific point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;&lt;/strong&gt;. &lt;strong&gt;Evaluating a function&lt;/strong&gt; &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; at a point &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is inner product of &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L_x\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(L_x \in \mathcal{H}_K\)&lt;/span&gt; is a &lt;strong&gt;evaluation functional&lt;/strong&gt; which is a kernal function and linear &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot, x)\)&lt;/span&gt;. Reproducing property of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_K\)&lt;/span&gt; can be achieved if all &lt;span class=&#34;math inline&#34;&gt;\(f \in \mathcal{H}\)&lt;/span&gt; has bounded evaluation functionals (&lt;span class=&#34;math inline&#34;&gt;\(L_x\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;In least square methods, the parameters (&lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt;) are determined by inner product of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta} = (X^{T}X)^{-1}X^{T}y\)&lt;/span&gt;. In Kernel method, &lt;span class=&#34;math inline&#34;&gt;\(\hat{\beta}\)&lt;/span&gt; is determined &lt;span class=&#34;math inline&#34;&gt;\(\langle K(\cdot,x_i), K(\cdot,x_j), \rangle_{\mathcal{H}_K} = K(x_i, x_j)\)&lt;/span&gt;. Each &lt;span class=&#34;math inline&#34;&gt;\(K(\cdot, x)\)&lt;/span&gt; is a parameter and a argument (variable like &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Some subclass of the loss function and penalty functions can be generated by a positive definite kernel. A Kernel accepts two arguments and a Kernel function does one argument and the other argument becomes parameter. Reproducing Kernel Hilbert space is a function space with Kernal function space with the evaluation functional as a Kernel. The feature expansion into the RKHS can use the Kernel matrix instead of the inner product of each variable &lt;span class=&#34;math inline&#34;&gt;\(X^TX\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The important concepts are Hilbert space, inner product, Kernel function, evaluation functional, feature expansion, Fourier transformation, Reisz representation theorem (dual space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_{K}^*\)&lt;/span&gt; of Hibert space &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{H}_K\)&lt;/span&gt;)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predictive Modeling</title>
      <link>/talk/predictive-modeling/</link>
      <pubDate>Tue, 10 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/talk/predictive-modeling/</guid>
      <description>



</description>
    </item>
    
  </channel>
</rss>
